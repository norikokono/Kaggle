{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":208024,"sourceType":"modelInstanceVersion","modelInstanceId":5388,"modelId":3533}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# üì∞ NewsBot: AI-Powered News Digest with Gemma\n\nWelcome to the **NewsBot** Notebook!\n\n> In today's fast-paced world, staying informed without being overwhelmed is a constant challenge. This notebook provides a hands-on solution by demonstrating how to build a fully automated system for generating a concise, insightful news digest.\n\nWe will harness the power of **Gemma** üß†, Google's family of lightweight, state-of-the-art open models, to perform sophisticated Natural Language Processing (NLP) tasks. The entire pipeline, from data fetching to final report, is handled right within this environment.\n\n---\n\n\n### üõ†Ô∏è Core Technologies of the first section\n\nThis project integrates several powerful tools to create the final digest:\n\n* **Data Source:** A dedicated **News API** connection üîó to fetch the latest English-language articles focused on \"Artificial Intelligence.\"\n* **NLP Model:** **Gemma**, deployed via the **KerasNLP** library, for performing abstractive summarization.\n* **Visualization:** Such as The **WordCloud**library üñºÔ∏è to generate visual representations of key news trends.\n* **Final Output:** A polished and shareable **HTML report** üìß.\n\n---\n\n### üöÄ The Digest Pipeline: From Raw Data to Final Report\n\nThis notebook guides you through each stage of the automated pipeline:\n\n1.  **üåê Data Acquisition**\n    * We'll connect to the News API and pull a fresh batch of articles related to **AI**.\n\n2.  **üß† Intelligent Summarization**\n    * You will use the Gemma model to condense lengthy articles into clear, digestible summaries. This process can be thought of as a function `$S(A) = s$`, where the model takes a full article `$A$` and produces a concise summary `$s$`.\n\n3.  **üìä Insight & Visualization**\n    * We'll analyze the summarized text to extract key themes and generate a compelling such as a **Word Cloud**, turning dense information into immediate visual insight.\n\n4.  **‚úÖ Report Generation**\n    * Finally, all the generated content‚Äîsummaries, trends, and the word cloud‚Äîwill be compiled into a single, polished **HTML news digest**.\n\nGet ready to transform your news consumption from a time-sink into a source of focused, **AI-driven insight**! üí°","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-17T18:33:14.423812Z","iopub.execute_input":"2025-10-17T18:33:14.424073Z","iopub.status.idle":"2025-10-17T18:33:15.244074Z","shell.execute_reply.started":"2025-10-17T18:33:14.424046Z","shell.execute_reply":"2025-10-17T18:33:15.243197Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q -U keras\n!pip install --upgrade keras-nlp\n!pip install --upgrade tensorflow","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T18:33:15.245786Z","iopub.execute_input":"2025-10-17T18:33:15.246217Z","iopub.status.idle":"2025-10-17T18:34:35.874158Z","shell.execute_reply.started":"2025-10-17T18:33:15.246195Z","shell.execute_reply":"2025-10-17T18:34:35.873465Z"},"_kg_hide-output":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip uninstall -y tensorflow-text\n!pip install tensorflow-text","metadata":{"trusted":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2025-10-17T18:34:35.875136Z","iopub.execute_input":"2025-10-17T18:34:35.875433Z","iopub.status.idle":"2025-10-17T18:35:18.326510Z","shell.execute_reply.started":"2025-10-17T18:34:35.875377Z","shell.execute_reply":"2025-10-17T18:35:18.325332Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip uninstall -y tensorflow-text\n\n# Downgrade TensorFlow and its addons to 2.18.0\n!pip install --upgrade --force-reinstall \\\n    tensorflow==2.18.0 \\\n    tensorboard==2.18.0 \\\n    tensorflow-text==2.18.0","metadata":{"trusted":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2025-10-17T18:35:18.327921Z","iopub.execute_input":"2025-10-17T18:35:18.328310Z","iopub.status.idle":"2025-10-17T18:36:22.255586Z","shell.execute_reply.started":"2025-10-17T18:35:18.328267Z","shell.execute_reply":"2025-10-17T18:36:22.254828Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Downgrade or reinstall gymnasium to the exact version required\n!pip install --upgrade --force-reinstall gymnasium==0.29.0","metadata":{"trusted":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2025-10-17T18:36:22.257850Z","iopub.execute_input":"2025-10-17T18:36:22.258051Z","iopub.status.idle":"2025-10-17T18:36:38.785009Z","shell.execute_reply.started":"2025-10-17T18:36:22.258029Z","shell.execute_reply":"2025-10-17T18:36:38.784285Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"os.environ[\"KERAS_BACKEND\"] = \"jax\"  # Or \"tensorflow\" or \"torch\".\nos.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.00\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T18:36:38.786302Z","iopub.execute_input":"2025-10-17T18:36:38.786741Z","iopub.status.idle":"2025-10-17T18:36:38.790333Z","shell.execute_reply.started":"2025-10-17T18:36:38.786713Z","shell.execute_reply":"2025-10-17T18:36:38.789815Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import requests\nimport time","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T18:36:38.791027Z","iopub.execute_input":"2025-10-17T18:36:38.791258Z","iopub.status.idle":"2025-10-17T18:36:38.866063Z","shell.execute_reply.started":"2025-10-17T18:36:38.791237Z","shell.execute_reply":"2025-10-17T18:36:38.865562Z"},"_kg_hide-output":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\n\nNEWS_API_KEY = UserSecretsClient().get_secret(\"NEWS_API_KEY\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T18:36:38.866683Z","iopub.execute_input":"2025-10-17T18:36:38.866872Z","iopub.status.idle":"2025-10-17T18:36:38.956139Z","shell.execute_reply.started":"2025-10-17T18:36:38.866855Z","shell.execute_reply":"2025-10-17T18:36:38.955461Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 1: Define Endpoint\n# url = f\"https://newsapi.org/v2/top-headlines?country=us&apiKey={NEWS_API_KEY}\"\n# url = f\"https://newsapi.org/v2/top-headlines?country=us&category=technology&apiKey={NEWS_API_KEY}\"\nurl = f\"https://newsapi.org/v2/everything?q=AI&language=en&apiKey={NEWS_API_KEY}\"\n\ndef fetch_news():\n    resp = requests.get(url)\n    resp.raise_for_status()\n    data = resp.json()\n    print(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} ‚Äì fetched {len(data['articles'])} articles\")\n    return data\n\n# always fetch on notebook start\nnews_data = fetch_news()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T18:36:38.956942Z","iopub.execute_input":"2025-10-17T18:36:38.957180Z","iopub.status.idle":"2025-10-17T18:36:39.242860Z","shell.execute_reply.started":"2025-10-17T18:36:38.957163Z","shell.execute_reply":"2025-10-17T18:36:39.242258Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom IPython.display import display, HTML # <-- Import for richer display\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\n# Assuming 'url' and 'requests' are defined/imported correctly in previous cells.\n\n# Step 2: Fetch News Data\nresponse = requests.get(url)\nresponse.raise_for_status() # Good practice to check for errors\nnews_data = response.json()\n\n# --- NEW CODE FOR BETTER PROCESSING AND DISPLAY ---\n\n# Step 3: Process News Articles\narticles = news_data.get(\"articles\", [])\ndf = pd.DataFrame(articles)\n\n# Step 4: Clean, Select, and Format Columns\ndf_clean = df[[\"source\", \"title\", \"description\", \"url\", \"publishedAt\"]].copy()\n\n# Extract the source name from the nested dictionary\ndf_clean[\"source\"] = df_clean[\"source\"].apply(lambda x: x[\"name\"] if isinstance(x, dict) else x)\n\n# Convert the ISO date string to a human-readable datetime object\ndf_clean[\"publishedAt\"] = pd.to_datetime(df_clean[\"publishedAt\"]).dt.strftime('%Y-%m-%d %H:%M')\n\n# Rename columns for presentation\ndf_clean.columns = [\"Source\", \"Title\", \"Description\", \"URL\", \"Published At\"]\n\n# Step 5: Display Prettier Output (using display() and HTML styling)\nprint(\"üì∞ Latest News Digest üì∞\")\nprint(\"------------------------\")\n\n# Use 'styler' to make the DataFrame look better in the notebook\ndisplay(\n    df_clean.head(10).style\n    .set_properties(**{'font-size': '10pt', 'border': '1px solid lightgrey'})\n    .set_table_styles([{'selector': 'th', 'props': [('background-color', '#f0f0f0')]}]),\n)\n\n# Optional: Save as CSV (if needed)\ndf_clean.to_csv(\"news_data.csv\", index=False)\n\n# Step 6: Visualization (Word Cloud of Titles)\nprint(\"\\n‚òÅÔ∏è Article Title Word Cloud ‚òÅÔ∏è\")\ntext = \" \".join(df_clean[\"Title\"].dropna())\nwordcloud = WordCloud(\n    width=1000, \n    height=500, \n    background_color=\"white\",\n    colormap=\"magma\", # Choose a nice color scheme\n    collocations=False # Helps with cleaner word separation\n).generate(text)\n\nplt.figure(figsize=(12, 6))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T18:36:39.243610Z","iopub.execute_input":"2025-10-17T18:36:39.243890Z","iopub.status.idle":"2025-10-17T18:36:40.702522Z","shell.execute_reply.started":"2025-10-17T18:36:39.243873Z","shell.execute_reply":"2025-10-17T18:36:40.701763Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ‚öôÔ∏è Generate and Export the News Digest (HTML File)\n\n> **To create the file `news_digest.html` in your notebook's output, run the cell below.** If you do not wish to generate or download the file, please skip this cell.\n\nOnce the cell completes, look for the file in your notebook's file browser/output pane.","metadata":{}},{"cell_type":"code","source":"#!/usr/bin/env python\n\"\"\"\nNews Digest Generator\n\nThis script loads a pretrained Gemma model (using keras_nlp) to analyze the latest AI-related news items.\nIt extracts key trends, ethical concerns, and future implications, then generates a Markdown-based digest\nthat includes a word cloud of news titles. The final output is converted into HTML and saved.\n\nRequirements:\n- `keras_nlp`, `tensorflow`, `pandas`, `markdown`, `wordcloud`, `matplotlib`\n\"\"\"\n\nfrom functools import lru_cache\nimport re\nimport keras\nimport keras_nlp\nimport tensorflow as tf\nfrom markdown import markdown\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nimport io\nimport base64\n\n# -----------------------------------------------------------------------------\n# IMPORTANT: Load your datasource and select the relevant columns.\n# You must define 'df' (for example via pd.read_csv(...)) before running the script.\n#\n# Step 4: Select Relevant Columns (using \"title\", \"description\", \"url\", \"publishedAt\")\ndf = df[[\"title\", \"description\", \"url\", \"publishedAt\"]]\n# -----------------------------------------------------------------------------\n\n# -----------------------------------------------------------------------------\n# 1. Load the Gemma Model\n# -----------------------------------------------------------------------------\nprecisions = [tf.bfloat16, None]\ngemma_lm = None\n\nfor precision in precisions:\n    try:\n        gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\n            \"gemma_instruct_2b_en\", jit_compile=False, dtype=precision\n        )\n        print(f\"‚úÖ Gemma model loaded successfully with {precision if precision else 'default'} precision.\")\n        break\n    except Exception as e:\n        print(f\"‚ùå Error loading Gemma model ({precision}): {e}\")\n\n# -----------------------------------------------------------------------------\n# 2. Define the AIAnalyzer Class\n# -----------------------------------------------------------------------------\nclass AIAnalyzer:\n    \"\"\"NewsBot: AI-powered analysis of AI-related news.\"\"\"\n    \n    def __init__(self, model, output_format=\"markdown\"):\n        self.model = model\n        self.output_format = output_format\n\n    @lru_cache(maxsize=50)\n    def analyze(self, news_item: str) -> dict:\n        \"\"\"\n        Generates an AI-powered analysis of a news item.\n        Expects news_item to be a string containing the title and description.\n        \"\"\"\n        prompt = f\"\"\"\nPlease analyze the following news item and generate an insightful breakdown of it. Do NOT repeat any part of this prompt or include extra instructions in your final response. Base your analysis solely on the information provided.\n\nNews item:\n{news_item}\n\nYour analysis MUST be exactly structured as follows, starting immediately with \"1. Key Trends\":\n1. Key Trends: Identify industry patterns, emerging technologies, risks, and opportunities evident in this news item.\n2. Implications: Discuss real-world applications and ethical concerns related to this news item.\n3. AI Insights: Provide an AI-driven perspective on what this news item means for the future.\n\nEnsure your response is well-organized, informative, and engaging.\n\"\"\"\n        try:\n            raw = self.model.generate(prompt, max_length=1024).strip()\n            response = (\n                raw[0] if isinstance(raw, (list, tuple))\n                else raw.numpy()[0].decode() if isinstance(raw, tf.Tensor)\n                else str(raw)\n            ).strip()\n        except Exception as e:\n            response = f\"‚ö†Ô∏è Error generating content: {e}\"\n\n        # --- Cleanup Routine ---\n\n        # 1. Trim any text preceding \"1. Key Trends:\" unconditionally.\n        marker_index = response.find(\"1. Key Trends:\")\n        if marker_index != -1:\n            response = response[marker_index:].strip()\n\n        # 2. Remove any trailing text starting from known prompt ending markers.\n        response = re.sub(r'\\n\\s*Ensure your response.*', '', response, flags=re.IGNORECASE)\n\n        # 3. Remove known unwanted instruction lines line-by-line.\n        unwanted_lines = {\n            \"1. Key Trends:\",\n            \"1. Key Trends: Identify industry patterns, emerging technologies, risks, and opportunities evident in this news item.\",\n            \"2. Implications: Discuss real-world applications and ethical concerns related to this news item.\",\n            \"3. AI Insights: Provide an AI-driven perspective on what this news item means for the future.\"\n        }\n        clean_lines = []\n        for line in response.splitlines():\n            if line.strip() not in unwanted_lines:\n                clean_lines.append(line)\n        response = \"\\n\".join(clean_lines).strip()\n\n        # 4. Reduce excessive newlines.\n        response = re.sub(r\"\\n{3,}\", \"\\n\\n\", response).strip()\n\n        # 5. If nothing remains (or if the cleaned response is empty), supply a fallback analysis.\n        if not response:\n            response = (\n                \"1. Key Trends: The news item demonstrates significant AI innovation with clear emerging trends that could reshape the industry. \"\n                \"2. Implications: It underscores both opportunities and critical risks, including ethical and practical challenges. \"\n                \"3. AI Insights: Overall, this development signals a transformative evolution in AI technology.\"\n            )\n\n        return {\"news_item\": news_item, \"response\": response}\n\n# -----------------------------------------------------------------------------\n# 3. Process the News Data\n# -----------------------------------------------------------------------------\n# Process the top 10 news items from the DataFrame.\nnews_df = df.head(10)[['title', 'description', 'url', 'publishedAt']]\n\nif gemma_lm is not None:\n    analyzer = AIAnalyzer(gemma_lm)\n    results = []\n    for _, row in news_df.iterrows():\n        title = row['title']\n        description = row['description']\n        # Construct the news item prompt using title and description.\n        news_item_prompt = f\"Title: {title}\\nDescription: {description}\"\n        analysis_result = analyzer.analyze(news_item_prompt)\n        analysis_result.update({\n            \"title\": title,\n            \"description\": description,\n            \"url\": row[\"url\"],\n            \"publishedAt\": row[\"publishedAt\"]\n        })\n        results.append(analysis_result)\nelse:\n    results = []\n    print(\"No model available; cannot generate news analysis.\")\n\n# -----------------------------------------------------------------------------\n# 4. Generate Word Cloud & Build Markdown Digest\n# -----------------------------------------------------------------------------\ntext = \" \".join(df[\"title\"].dropna())\nwordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate(text)\n\n# Save the word cloud image to a BytesIO object and encode as base64.\nbuf = io.BytesIO()\nplt.figure(figsize=(10, 5))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.tight_layout()\nplt.savefig(buf, format='png', bbox_inches='tight')\nplt.close()\n\nimage_base64 = base64.b64encode(buf.getvalue()).decode('utf-8')\nimage_html = f'<img src=\"data:image/png;base64,{image_base64}\" alt=\"Word Cloud\" style=\"max-width: 100%; height: auto;\">'\n\n# -----------------------------------------------------------------------------\n# 5. Build Markdown Output and Convert to HTML\n# -----------------------------------------------------------------------------\nmarkdown_output = \"# üåüüîé AI-Powered News Digest üî•\\n\\n\"\nmarkdown_output += image_html + \"\\n\\n\"\n\nfor i, rep in enumerate(results, start=1):\n    markdown_piece = f\"\"\"\n---\n## {i}. üì∞ **{rep['title']}**\n\n**Description:** <br>\n{rep['description']}\n\nüïí **Published At:** {rep['publishedAt']} <br>\nüîó **URL:** [{rep['url']}]({rep['url']}) <br>\n\n{rep['response']}\n\"\"\"\n    markdown_output += markdown_piece\n\nhtml_content = markdown(markdown_output)\n\nhtml_page = f\"\"\"<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <title>AI-Powered News Digest</title>\n    <style>\n        body {{\n            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n            line-height: 1.6;\n            padding: 20px;\n            max-width: 900px;\n            margin: auto;\n            background-color: #f9f9f9; /* Added for a light background */\n        }}\n        \n        h1 {{\n            color: #1f4d49;\n            text-align: center;\n            border-bottom: 2px solid #1f4d49; /* Updated for a stronger header line */\n            padding-bottom: 10px;\n        }}\n        \n        h2 {{\n            color: #2a647d; /* Changed header color */\n            border-bottom: 1px solid #ddd;\n            padding-bottom: 5px;\n            margin-top: 30px;\n        }}\n    \n        a {{\n            color: #007bff; /* Changed link color */\n            text-decoration: none;\n        }}\n        \n        a:hover {{\n            text-decoration: underline;\n        }}\n        \n        hr {{\n            margin: 40px 0;\n            border: 0;\n            border-top: 5px dotted #e0e0e0; /* Changed to a dotted separator */\n        }}\n        \n        img {{\n            display: block;\n            margin: 20px auto;\n            border: 1px solid #ddd; /* Added border and radius for images */\n            border-radius: 5px;\n        }}\n    </style>\n</head>\n<body>\n{html_content}\n</body>\n</html>\n\"\"\"\n\nwith open(\"news_digest.html\", \"w\", encoding=\"utf-8\") as f:\n    f.write(html_page)\n\n# 1. Print the success message\nprint(\"‚úÖ Page generated successfully! Open 'news_digest.html' in your browser to view your AI-powered news digest.\")\n\n# 2. Display the HTML content directly in the notebook output cell\nprint(\"\\n--- Displaying HTML Output Below ---\")\ndisplay(HTML(html_page))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T18:36:40.703436Z","iopub.execute_input":"2025-10-17T18:36:40.703673Z","iopub.status.idle":"2025-10-17T18:43:11.928967Z","shell.execute_reply.started":"2025-10-17T18:36:40.703653Z","shell.execute_reply":"2025-10-17T18:43:11.928146Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"background-color: #F0F8FF; padding: 20px; border-radius: 10px; border: 1px solid #D0E6FA; font-family: Arial, sans-serif; line-height: 1.6; color: #333;\">\n  \n  <h2 style=\"margin-top: 0;\">üì∞ AI News Summary with Gemma LLM</h2>\n\n  <p>Stay up to date with the most popular <strong>AI</strong> and <strong>LLM</strong> news (excluding anything related to <em>crypto</em>), automatically analyzed using a <strong>Gemma large language model</strong>.</p>\n\n  <hr style=\"border: none; border-top: 1px solid #D0E6FA; margin: 20px 0;\">\n\n  <h3>‚öôÔ∏è How It Works:</h3>\n  <ul style=\"padding-left: 20px;\">\n    <li>üîó <strong>Fetch News:</strong> Pulls articles from NewsAPI using keywords like <code>AI</code> or <code>LLM</code> (excluding <code>crypto</code>).</li>\n    <li>üß† <strong>Analyze with LLM:</strong> Summarizes using the Gemma model.</li>\n    <li>üìä <strong>Display Results:</strong> Presents data in a clean, styled table including key metadata.</li>\n  </ul>\n\n  <h3>‚úÖ What You Get:</h3>\n  <p>A streamlined table featuring the <strong>Top 5 AI-related articles</strong> with:</p>\n  <ul style=\"padding-left: 20px;\">\n    <li>‚úçÔ∏è <strong>Title</strong></li>\n    <li>üì∞ <strong>LLM-generated Summary</strong></li>\n    <li>üè∑Ô∏è <strong>Source</strong></li>\n    <li>üìÖ <strong>Published Date</strong></li>\n    <li>üîó <strong>Direct Link to Full Article</strong></li>\n  </ul>\n\n</div>\n","metadata":{}},{"cell_type":"code","source":"# Constants\nNEWS_QUERY = \"(AI OR LLM) NOT crypto\"\nNEWS_SORT = \"popularity\"\nNEWS_LANG = \"en\"\nNEWS_PAGE_SIZE = 10\n\n# Your API key must be set as an environment variable or manually provided\nurl = (\n    f\"https://newsapi.org/v2/everything\"\n    f\"?q={NEWS_QUERY}&language={NEWS_LANG}&sortBy={NEWS_SORT}\"\n    f\"&pageSize={NEWS_PAGE_SIZE}&apiKey={NEWS_API_KEY}\"\n)\n\n# Analysis & Display Settings\nLLM_TOP_COUNT = 5\nDISPLAY_COUNT = 5\n\n# --- Gemma Analysis Function ---\ndef analyze_article_with_gemma(title, description):\n    \"\"\"\n    Generate a concise summary and sentiment using the Gemma LLM.\n    \"\"\"\n    if 'gemma_lm' not in globals() or gemma_lm is None:\n        return \"LLM Not Loaded: Please initialize 'gemma_lm'.\"\n\n    prompt = f\"\"\"\n    Analyze the following news article snippet and generate a summary \n    including its main topic and overall sentiment (positive, negative, or neutral).\n\n    Title: \"{title}\"\n    Snippet: \"{description}\"\n    \n    Summary:\n    \"\"\"\n\n    try:\n        output = gemma_lm.generate(prompt, max_length=384)\n        summary = output.split(\"Summary:\")[-1].strip().split('\\n')[0].replace('\"', '')\n        return summary\n    except Exception as e:\n        return f\"LLM Analysis Error: {e}\"\n\n# --- Fetch Articles ---\ndef fetch_articles():\n    try:\n        print(\"üì° Fetching news data...\")\n        response = requests.get(url)\n        response.raise_for_status()\n        return response.json().get('articles', [])\n    except requests.exceptions.RequestException as e:\n        print(f\"‚ùå Request Error: {e}\")\n        return []\n\n# --- Main Display Logic ---\ndef process_and_display_articles(articles):\n    if not articles:\n        print(\"No articles available to display.\")\n        return\n\n    print(f\"üåü Displaying Top {DISPLAY_COUNT} AI/LLM Articles with LLM Analysis üåü\\n\")\n\n    display_data = []\n\n    for i, article in enumerate(articles[:DISPLAY_COUNT]):\n        title = article.get('title', 'N/A')\n        description = article.get('description', 'No description available.')\n        source = article.get('source', {}).get('name', 'Unknown Source')\n        date = article.get('publishedAt', '')[:10] or 'N/A'\n        link = article.get('url', '#')\n\n        llm_summary = \"N/A\"\n        if i < LLM_TOP_COUNT:\n            print(f\"üß† Analyzing Article {i+1} with Gemma...\")\n            llm_summary = analyze_article_with_gemma(title, description)\n\n        display_data.append({\n            \"Rank\": i + 1,\n            \"LLM Analysis\": llm_summary,\n            \"Title\": title,\n            \"Source\": source,\n            \"Date\": date,\n            \"Link\": f'<a href=\"{link}\" target=\"_blank\">Read Article</a>'\n        })\n\n    df = pd.DataFrame(display_data)\n\n    styled_df = df.style.set_table_styles([\n        {'selector': 'th', 'props': [\n            ('background-color', '#1E88E5'),\n            ('color', '#fff'),\n            ('font-size', '14px'),\n            ('text-align', 'center'),\n            ('padding', '12px 8px'),\n            ('border', '1px solid #BBDEFB')\n        ]},\n        {'selector': 'td', 'props': [\n            ('font-family', 'Arial, sans-serif'),\n            ('padding', '10px 8px'),\n            ('border', '1px solid #E0E0E0'),\n            ('vertical-align', 'top')\n        ]},\n        {'selector': 'tr:nth-child(even)', 'props': [\n            ('background-color', '#F9FBFD')\n        ]},\n        {'selector': 'tr:hover', 'props': [\n            ('background-color', '#E3F2FD')\n        ]},\n        {'selector': 'table', 'props': [\n            ('border-collapse', 'collapse'),\n            ('width', '100%'),\n            ('box-shadow', '0 2px 5px rgba(0, 0, 0, 0.1)')\n        ]},\n        {'selector': 'a', 'props': [\n            ('color', '#1E88E5'),\n            ('text-decoration', 'none'),\n            ('font-weight', 'bold')\n        ]}\n    ]).set_properties(\n        subset=['Rank', 'Date', 'Link'], **{'text-align': 'center'}\n    ).set_properties(\n        subset=['LLM Analysis', 'Title'], **{'text-align': 'left', 'max-width': '400px'}\n    ).hide(axis='index').set_caption(\n        f\"üì∞ Top {DISPLAY_COUNT} AI Articles (Analyzed by Gemma LLM)\"\n    )\n\n    display(HTML(styled_df.to_html(escape=False)))\n\n# --- Run Pipeline ---\narticles = fetch_articles()\nprocess_and_display_articles(articles)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T18:43:11.930085Z","iopub.execute_input":"2025-10-17T18:43:11.930413Z","iopub.status.idle":"2025-10-17T18:44:25.625390Z","shell.execute_reply.started":"2025-10-17T18:43:11.930392Z","shell.execute_reply":"2025-10-17T18:44:25.624616Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üöÄ LLM Ethical News Dashboard Pipeline Summary \n\nThis document summarizes the execution of a **three-phase data pipeline** designed to analyze current news trends regarding the ethical implications of Large Language Models (LLMs), culminating in three dynamic, interactive visualizations.\n\n---\n\n### 1. üêç Phase 1: Data Acquisition (News API / Python)\n\n* **Action:** Used standard Python libraries (`requests`) to execute a **paginated fetch loop** against the News API, which automatically iterates through all pages to retrieve the **complete, available dataset** over the last **7 days**.\n* **Query:** The search used a complex, targeted query focusing on LLMs AND specific ethical topics:\n    * **LLM Keywords:** `\"large language model\" OR LLM OR ChatGPT OR Gemini`\n    * **Ethical Focus:** `accountability OR \"intellectual property\" OR \"job displacement\" OR \"data privacy\" OR \"environmental impact\"`\n    * **Exclusions:** Excluded common noise terms like `NOT (bias OR hallucination)`.\n* **Result:** A complete list of raw news article dictionaries (`raw_articles`), with a size **dynamically determined** by the API, ready for LLM processing.\n\n---\n\n### 2. ü§ñ Phase 2: LLM Data Structuring (GemmaCausalLM)\n\n* **Goal:** To transform raw, unstructured text (titles/descriptions) into clean, categorical data required for plotting.\n* **Tool:** The specialized Keras-NLP model, **GemmaCausalLM** (`gemma_lm`), was integrated for classification.\n* **Process:** A dedicated Python function runs the `gemma_lm` against each article using a **carefully engineered prompt**. This prompt forces the LLM to perform three core tasks and output a **strict JSON object** for each article:\n    * `application`: Classifies the primary LLM industry (e.g., 'Healthcare/Science').\n    * `concerns`: Identifies all relevant ethical issues (e.g., \\[\"Data Privacy\", \"Accountability & IP\"]).\n    * `entities`: Extracts the key organizations/products (e.g., \\[\"Google\", \"OpenAI\"]).\n* **Result:** A clean Pandas DataFrame (`df`) containing the structured, classified data, ready for aggregation.\n\n---\n\n### 3. üìä Phase 3: Visualization (Pandas & Plotly)\n\nThe structured DataFrame was aggregated using **Pandas** and visualized using the **Plotly** library to create three interactive charts that provide comprehensive analysis:\n\n1.  **üìà Line Chart (Trend Over Time):** Shows the **daily volume** of news coverage for each ethical concern category, highlighting trending issues.\n2.  **ü•ß Pie Chart (Overall Distribution):** Displays the **total percentage share** of each ethical concern across the full dataset.\n3.  **üìä Stacked Bar Chart (Entity vs. Application):** Links the **Top 7 Entities** to the specific **Application Areas** they are most frequently associated with in the news.","metadata":{}},{"cell_type":"code","source":"# ============================\n# üì¶ Imports & Setup\n# ============================\nimport os, re, json, math, requests\nimport pandas as pd\nimport plotly.express as px\nfrom tqdm import tqdm\nfrom datetime import datetime, timedelta, timezone\n\nimport plotly.io as pio\npio.renderers.default = \"iframe\"   # often the most reliable in Kaggle\n\n\n# Make sure you set your News API key in Kaggle environment variables\n# NEWS_API_KEY = os.getenv(\"NEWS_API_KEY\")\nBASE_URL = \"https://newsapi.org/v2/everything\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T18:44:25.626262Z","iopub.execute_input":"2025-10-17T18:44:25.626650Z","iopub.status.idle":"2025-10-17T18:44:26.212009Z","shell.execute_reply.started":"2025-10-17T18:44:25.626623Z","shell.execute_reply":"2025-10-17T18:44:26.211238Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================\n# üêç Phase 1: Data Acquisition\n# ============================\n\ndef build_query():\n    must_llm = '(\"large language model\" OR LLM OR ChatGPT OR Gemini)'\n    must_ethics = '(accountability OR \"intellectual property\" OR \"job displacement\" OR \"data privacy\" OR \"environmental impact\")'\n    exclusions = 'NOT (bias OR hallucination)'\n    return f'{must_llm} AND {must_ethics} AND {exclusions}'\n\ndef iso_day_range(days=7):\n    now = datetime.now(timezone.utc)\n    from_date = (now - timedelta(days=days)).replace(microsecond=0)\n    return from_date.isoformat(), now.replace(microsecond=0).isoformat()\n\ndef fetch_all_articles(page_size=100, max_pages=None):\n    q = build_query()\n    from_iso, to_iso = iso_day_range(7)\n\n    params = {\n        \"q\": q,\n        \"from\": from_iso,\n        \"to\": to_iso,\n        \"language\": \"en\",\n        \"sortBy\": \"publishedAt\",\n        \"pageSize\": page_size,\n        \"apiKey\": NEWS_API_KEY,\n    }\n\n    resp = requests.get(BASE_URL, params=params, timeout=30)\n    resp.raise_for_status()\n    data = resp.json()\n\n    total = data.get(\"totalResults\", 0)\n    pages = math.ceil(total / page_size)\n    if max_pages is not None:\n        pages = min(pages, max_pages)\n\n    articles = data.get(\"articles\", [])\n    for page in tqdm(range(2, pages + 1), desc=\"Fetching pages\"):\n        params[\"page\"] = page\n        r = requests.get(BASE_URL, params=params, timeout=30)\n        r.raise_for_status()\n        j = r.json()\n        articles.extend(j.get(\"articles\", []))\n\n    seen, raw_articles = set(), []\n    for a in articles:\n        url = a.get(\"url\")\n        if not url or url in seen:\n            continue\n        seen.add(url)\n        raw_articles.append({\n            \"title\": a.get(\"title\") or \"\",\n            \"description\": a.get(\"description\") or \"\",\n            \"source\": (a.get(\"source\") or {}).get(\"name\") or \"\",\n            \"url\": url,\n            \"publishedAt\": a.get(\"publishedAt\") or \"\",\n        })\n    return raw_articles\n\n# Example run\n# raw_articles = fetch_all_articles(max_pages=2)\n# print(len(raw_articles))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T18:44:26.214517Z","iopub.execute_input":"2025-10-17T18:44:26.214732Z","iopub.status.idle":"2025-10-17T18:44:26.223412Z","shell.execute_reply.started":"2025-10-17T18:44:26.214716Z","shell.execute_reply":"2025-10-17T18:44:26.222698Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================\n# ü§ñ Phase 2: Classification\n# ============================\n\nCLOSED_APPLICATIONS = {\n    \"Healthcare/Science\",\"Education\",\"Finance\",\"Government/Public Sector\",\n    \"Enterprise/Work\",\"Consumer\",\"Media/Entertainment\",\"Research\",\"Legal\"\n}\nCLOSED_CONCERNS = {\"Data Privacy\",\"Accountability & IP\",\"Job Displacement\",\"Environmental Impact\",\"Other\"}\n\ndef run_gemma(prompt: str, max_len: int = 512) -> str:\n    \"\"\"Use the already-imported gemma_lm to generate text.\"\"\"\n    output = gemma_lm.generate(prompt, max_length=max_len)\n    if isinstance(output, dict) and \"text\" in output:\n        return output[\"text\"]\n    elif isinstance(output, list):\n        return output[0].get(\"text\", \"\")\n    return str(output)\n\ndef extract_json(text: str) -> dict:\n    match = re.search(r\"\\{.*\\}\", text, flags=re.DOTALL)\n    if not match:\n        return {}\n    try:\n        return json.loads(match.group(0))\n    except json.JSONDecodeError:\n        cleaned = re.sub(r\",\\s*}\", \"}\", match.group(0))\n        cleaned = re.sub(r\",\\s*]\", \"]\", cleaned)\n        try:\n            return json.loads(cleaned)\n        except:\n            return {}\n\ndef normalize_fields(obj: dict) -> dict:\n    app = obj.get(\"application\", \"\")\n    concerns = obj.get(\"concerns\", [])\n    entities = obj.get(\"entities\", [])\n\n    if app not in CLOSED_APPLICATIONS:\n        app = \"Other\"\n\n    if not isinstance(concerns, list):\n        concerns = [concerns]\n    concerns = [c for c in concerns if c in CLOSED_CONCERNS] or [\"Other\"]\n\n    if not isinstance(entities, list):\n        entities = [entities]\n    entities = [e for e in entities if isinstance(e, str) and e.strip()] or [\"Other\"]\n\n    return {\"application\": app, \"concerns\": concerns, \"entities\": entities}\n\ndef classify_articles(raw_articles):\n    rows = []\n    for a in tqdm(raw_articles, desc=\"Classifying\"):\n        prompt = f\"\"\"\nYou are classifying news articles about the ethical implications of Large Language Models.\n\nReturn ONLY a valid JSON object with keys: application, concerns, entities.\n\nRules:\n- application: choose ONE from {list(CLOSED_APPLICATIONS)}\n- concerns: choose ALL that apply from {list(CLOSED_CONCERNS)}\n- entities: list key organizations/products mentioned explicitly. If unknown, use \"Other\".\n\nTITLE: {a['title']}\nDESCRIPTION: {a['description']}\n\"\"\"\n        out = run_gemma(prompt)\n        obj = extract_json(out)\n        norm = normalize_fields(obj)\n        rows.append({\n            \"title\": a[\"title\"],\n            \"description\": a[\"description\"],\n            \"source\": a[\"source\"],\n            \"url\": a[\"url\"],\n            \"publishedAt\": a[\"publishedAt\"],\n            \"application\": norm[\"application\"],\n            \"concerns\": norm[\"concerns\"],\n            \"entities\": norm[\"entities\"],\n        })\n    df = pd.DataFrame(rows)\n    df = df.explode(\"concerns\").reset_index(drop=True)\n    df[\"date\"] = pd.to_datetime(df[\"publishedAt\"]).dt.date\n    return df\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T18:44:26.224175Z","iopub.execute_input":"2025-10-17T18:44:26.224412Z","iopub.status.idle":"2025-10-17T18:44:26.237343Z","shell.execute_reply.started":"2025-10-17T18:44:26.224390Z","shell.execute_reply":"2025-10-17T18:44:26.236677Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================\n# üìä Phase 3: Visualization\n# ============================\n\ndef plot_trend_over_time(df):\n    trend = (df.groupby([\"date\", \"concerns\"])\n               .size()\n               .reset_index(name=\"count\"))\n    fig = px.line(\n        trend,\n        x=\"date\",\n        y=\"count\",\n        color=\"concerns\",\n        markers=True,\n        title=\"Daily news volume by ethical concern\"\n    )\n    fig.show()\n\ndef plot_overall_distribution(df):\n    dist = df.groupby(\"concerns\").size().reset_index(name=\"count\")\n    fig = px.pie(\n        dist,\n        names=\"concerns\",\n        values=\"count\",\n        title=\"Overall distribution of ethical concerns\"\n    )\n    fig.update_traces(textposition='inside', textinfo='percent+label')\n    fig.show()\n\ndef plot_entities_vs_application(df, top_n=7):\n    df_ent = df.explode(\"entities\")\n    top_entities = df_ent[\"entities\"].value_counts().head(top_n).index.tolist()\n    filtered = df_ent[df_ent[\"entities\"].isin(top_entities)]\n    pivot = (filtered.groupby([\"entities\", \"application\"])\n             .size()\n             .reset_index(name=\"count\"))\n    fig = px.bar(\n        pivot,\n        x=\"entities\",\n        y=\"count\",\n        color=\"application\",\n        title=f\"Top {top_n} entities by associated application areas\",\n        barmode=\"stack\"\n    )\n    fig.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T18:44:26.238012Z","iopub.execute_input":"2025-10-17T18:44:26.238202Z","iopub.status.idle":"2025-10-17T18:44:26.249607Z","shell.execute_reply.started":"2025-10-17T18:44:26.238188Z","shell.execute_reply":"2025-10-17T18:44:26.248939Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================\n# üöÄ Run the Full Pipeline\n# ============================\n\n# 1. Acquire\nraw_articles = fetch_all_articles(max_pages=2)  # limit pages for demo\nprint(f\"Fetched {len(raw_articles)} articles\")\n\n# 2. Classify\ndf = classify_articles(raw_articles)\nprint(df.head())\n\n# 3. Visualize\nplot_trend_over_time(df)\nplot_overall_distribution(df)\nplot_entities_vs_application(df)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T18:44:26.250237Z","iopub.execute_input":"2025-10-17T18:44:26.250477Z","iopub.status.idle":"2025-10-17T18:55:30.999854Z","shell.execute_reply.started":"2025-10-17T18:44:26.250458Z","shell.execute_reply":"2025-10-17T18:55:30.999135Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"üìä Generate and display the charts","metadata":{}},{"cell_type":"code","source":"# 1. Line chart: daily volume per ethical concern\ntrend = df.groupby([\"date\", \"concerns\"]).size().reset_index(name=\"count\")\nfig1 = px.line(trend, x=\"date\", y=\"count\", color=\"concerns\", markers=True,\n               title=\"üìà Daily News Volume by Ethical Concern\")\nfig1.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T18:55:31.000600Z","iopub.execute_input":"2025-10-17T18:55:31.000853Z","iopub.status.idle":"2025-10-17T18:55:31.138241Z","shell.execute_reply.started":"2025-10-17T18:55:31.000835Z","shell.execute_reply":"2025-10-17T18:55:31.137381Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 2. Pie chart: overall distribution of ethical concerns\ndist = df[\"concerns\"].value_counts().reset_index()\ndist.columns = [\"concerns\", \"count\"]\nfig2 = px.pie(dist, names=\"concerns\", values=\"count\",\n              title=\"ü•ß Overall Distribution of Ethical Concerns\")\nfig2.update_traces(textposition='inside', textinfo='percent+label')\nfig2.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T18:55:31.139031Z","iopub.execute_input":"2025-10-17T18:55:31.139346Z","iopub.status.idle":"2025-10-17T18:55:31.253340Z","shell.execute_reply.started":"2025-10-17T18:55:31.139328Z","shell.execute_reply":"2025-10-17T18:55:31.252767Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 3. Stacked bar chart: top entities vs application areas\ndf_exp = df.explode(\"entities\")\ntop_entities = df_exp[\"entities\"].value_counts().head(7).index.tolist()\nfiltered = df_exp[df_exp[\"entities\"].isin(top_entities)]\npivot = filtered.groupby([\"entities\", \"application\"]).size().reset_index(name=\"count\")\n\nfig3 = px.bar(pivot, x=\"entities\", y=\"count\", color=\"application\",\n              title=\"üìä Top Entities vs Application Areas\", barmode=\"stack\")\nfig3.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T18:55:31.254084Z","iopub.execute_input":"2025-10-17T18:55:31.254345Z","iopub.status.idle":"2025-10-17T18:55:31.382681Z","shell.execute_reply.started":"2025-10-17T18:55:31.254321Z","shell.execute_reply":"2025-10-17T18:55:31.381940Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# üì∞ Automated News Analysis: From Headlines to Actionable Insights\n\n## üöÄ Introduction: Uncovering Trends with NewsAPI and Sentiment Analysis\n\nWelcome to this core section of the automated news analysis project! The goal here is to transform raw news headlines into **actionable intelligence** on any given topic.\n\n---\n\n### **Pipeline Overview**\n\n1.  **Dynamic Data Retrieval (NewsAPI):**\n    * We use the **NewsAPI** with Python to dynamically fetch a high-volume stream of the latest global news articles related to a specified keyword (e.g., 'Artificial Intelligence', 'Climate Policy', 'Market Volatility').\n\n2.  **Quantifying the Mood (VADER Sentiment Analysis):**\n    * The VADER (Valence Aware Dictionary and sEntiment Reasoner) library is applied to the headline and article text. VADER is specifically attuned to social media and general text, making it highly effective for news analysis.\n    * It quantifies the prevailing media mood, assigning a **Compound Sentiment Score** (ranging from -1.0 for extremely negative to +1.0 for extremely positive) to each article.\n\n---\n\n### **üéØ Deliverables: The Power of the Output**\n\nThe combination of data retrieval and sentiment scoring generates highly readable outputs:\n\n* **A Comprehensive DataFrame:** A clean, organized table showing the news source, headline, publish date, and the calculated sentiment scores (Positive, Negative, Neutral, and Compound).\n* **Intuitive Visualization:** A chart illustrating the sentiment distribution over time and across news sources.\n\nThis snapshot of media coverage helps us quickly identify:\n\n| Insight | Description |\n| :--- | :--- |\n| **Emerging Trends** | Spikes in coverage (volume) combined with highly positive sentiment. |\n| **Risk Areas** | Consistent, high-volume coverage with pronounced negative sentiment. |\n| **Market Excitement** | Sudden, strong positive sentiment correlated with a key event. |","metadata":{}},{"cell_type":"code","source":"# 1. Installation and Imports\n!pip install newsapi-python vaderSentiment\nimport pandas as pd\nfrom newsapi import NewsApiClient\nfrom kaggle_secrets import UserSecretsClient\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import display, HTML\n\n# --- üí° USER INPUT CONFIGURATION ---\n# Define the search term here.\nsearch_query = \"AI or artificial intelligence\"\n# Define the desired number of articles to fetch per page (max 100 on free tier)\nPAGE_SIZE = 100\n# Define the language (e.g., 'en', 'de', 'fr')\nLANGUAGE = 'en'\n# ------------------------------------\n\nprint(f\"Configuration set: Analyzing news for '{search_query}'...\")\n\n# 2. Secure API Client Initialization\nprint(\"Initializing NewsAPI Client...\")\ntry:\n    secret_client = UserSecretsClient()\n    api_key = secret_client.get_secret(\"NEWS_API_KEY\")\n    newsapi = NewsApiClient(api_key=api_key)\n    print(\"API Client successfully initialized.\")\nexcept Exception as e:\n    print(f\"Error accessing secret: {e}. Please ensure 'NEWS_API_KEY' is set in Kaggle Secrets.\")\n    # Use placeholder to allow subsequent cells to run, but with an empty article list\n    articles = []\n    newsapi = None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T18:55:31.383591Z","iopub.execute_input":"2025-10-17T18:55:31.384024Z","iopub.status.idle":"2025-10-17T18:55:34.826166Z","shell.execute_reply.started":"2025-10-17T18:55:31.383994Z","shell.execute_reply":"2025-10-17T18:55:34.825483Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 3. Fetch News Articles\narticles = []\nif newsapi:\n    print(f\"Fetching articles for query: '{search_query}'...\")\n    try:\n        all_articles = newsapi.get_everything(\n            q=search_query,\n            language=LANGUAGE,\n            sort_by='relevancy',\n            page_size=PAGE_SIZE\n        )\n        articles = all_articles['articles']\n        total_results = all_articles['totalResults']\n        print(f\"Successfully retrieved {len(articles)} articles (Total found: {total_results}).\")\n    except Exception as e:\n        print(f\"An error occurred during the API call: {e}\")\n        articles = []\n\nif not articles:\n    print(\"\\n‚ö†Ô∏è No articles retrieved. Check your API key and query.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T18:55:34.827222Z","iopub.execute_input":"2025-10-17T18:55:34.827806Z","iopub.status.idle":"2025-10-17T18:55:35.163689Z","shell.execute_reply.started":"2025-10-17T18:55:34.827778Z","shell.execute_reply":"2025-10-17T18:55:35.163116Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 4. Data Processing and DataFrame Creation\nif articles:\n    news_df = pd.DataFrame(articles)\n    \n    # Extract source name\n    news_df['source_name'] = news_df['source'].apply(lambda x: x['name'] if isinstance(x, dict) and 'name' in x else 'Unknown')\n    news_df.drop('source', axis=1, inplace=True) \n    \n    # Convert date to datetime object\n    news_df['publishedAt'] = pd.to_datetime(news_df['publishedAt'], utc=True)\n    \n    # Select and rename relevant columns\n    news_df = news_df[['publishedAt', 'source_name', 'title', 'description', 'url', 'content']]\n    news_df.rename(columns={'publishedAt': 'Date', 'source_name': 'Source', 'title': 'Title', 'description': 'Description', 'url': 'URL'}, inplace=True)\n    \n    print(\"DataFrame successfully created and cleaned.\")\n    print(f\"DataFrame Shape: {news_df.shape}\")\nelse:\n    print(\"Skipping DataFrame creation as no articles were found.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T18:55:35.164305Z","iopub.execute_input":"2025-10-17T18:55:35.164519Z","iopub.status.idle":"2025-10-17T18:55:35.177797Z","shell.execute_reply.started":"2025-10-17T18:55:35.164495Z","shell.execute_reply":"2025-10-17T18:55:35.177002Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 5. Sentiment Analysis (VADER)\nif 'news_df' in locals() and not news_df.empty:\n    analyzer = SentimentIntensityAnalyzer()\n    \n    def get_sentiment_score(text):\n        \"\"\"Analyzes sentiment for a given text and returns the VADER compound score.\"\"\"\n        if pd.isna(text) or text is None or str(text) == 'None':\n            return 0.0\n        # Use description for sentiment\n        return analyzer.polarity_scores(str(text))['compound']\n\n    def categorize_sentiment(score):\n        \"\"\"Categorizes the VADER compound score.\"\"\"\n        if score >= 0.05:\n            return 'Positive'\n        elif score <= -0.05:\n            return 'Negative'\n        else:\n            return 'Neutral'\n\n    # Apply sentiment analysis\n    news_df['Sentiment_Score'] = news_df['Description'].apply(get_sentiment_score)\n    news_df['Sentiment'] = news_df['Sentiment_Score'].apply(categorize_sentiment)\n    \n    print(\"Sentiment analysis complete.\")\nelse:\n    print(\"Skipping sentiment analysis.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T18:55:35.178556Z","iopub.execute_input":"2025-10-17T18:55:35.178737Z","iopub.status.idle":"2025-10-17T18:55:35.213701Z","shell.execute_reply.started":"2025-10-17T18:55:35.178723Z","shell.execute_reply":"2025-10-17T18:55:35.213120Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 6. Styled Data Preview for Readability\n\nif 'news_df' in locals() and not news_df.empty:\n    \n    # Define a helper function to color sentiment scores\n    def color_sentiment(val):\n        \"\"\"Applies a color gradient based on the sentiment score.\"\"\"\n        if val >= 0.05:\n            # Green for Positive\n            color = '#90EE90' \n        elif val <= -0.05:\n            # Light Red for Negative\n            color = '#F08080' \n        else:\n            # Neutral/Grey\n            color = '#D3D3D3'\n        return f'background-color: {color}'\n\n    # Prepare DataFrame for display: show only key columns\n    display_cols = ['Date', 'Source', 'Title', 'Sentiment', 'Sentiment_Score']\n    styled_df = news_df[display_cols].head(10).style \\\n        .set_caption(f\"Top 10 News Articles for: '{search_query.title()}'\") \\\n        .applymap(color_sentiment, subset=['Sentiment_Score']) \\\n        .background_gradient(cmap='viridis', subset=['Sentiment_Score']) \\\n        .format({'Date': lambda t: t.strftime('%Y-%m-%d %H:%M'), 'Sentiment_Score': \"{:.3f}\"}) \\\n        .set_properties(**{'font-size': '10pt', 'border-color': 'lightgrey'}) \\\n        .hide(axis='index') # Hides the default index column\n        \n    display(styled_df)\nelse:\n    print(\"Cannot display data: DataFrame is empty.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T18:55:35.214346Z","iopub.execute_input":"2025-10-17T18:55:35.214551Z","iopub.status.idle":"2025-10-17T18:55:35.231551Z","shell.execute_reply.started":"2025-10-17T18:55:35.214537Z","shell.execute_reply":"2025-10-17T18:55:35.230825Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 7. Visualization: Sentiment Distribution\n\nif 'news_df' in locals() and not news_df.empty:\n    plt.figure(figsize=(8, 5))\n    \n    # Define colors for the chart\n    color_map = {'Positive': '#4CAF50', 'Neutral': '#FFC107', 'Negative': '#F44336'}\n    \n    sns.countplot(x='Sentiment', \n                  data=news_df, \n                  order=['Positive', 'Neutral', 'Negative'], \n                  palette=color_map)\n    \n    plt.title(f'Sentiment Distribution for News on: \"{search_query.title()}\"', \n              fontsize=14, \n              fontweight='bold')\n    plt.xlabel('Sentiment Category', fontsize=12)\n    plt.ylabel('Number of Articles', fontsize=12)\n    plt.grid(axis='y', linestyle='--', alpha=0.7)\n    plt.show()\n    \n    # Summary box\n    positive_count = news_df[news_df['Sentiment'] == 'Positive'].shape[0]\n    total_count = news_df.shape[0]\n    positive_percent = (positive_count / total_count) * 100 if total_count > 0 else 0\n    \n    display(HTML(f\"\"\"\n    <div style=\"border: 1px solid #007BFF; padding: 10px; border-radius: 5px; background-color: #E6F0FF;\">\n        <p><strong>üìä Analysis Summary:</strong></p>\n        <ul>\n            <li>**Total Articles:** {total_count}</li>\n            <li>**Positive Articles:** {positive_count} ({positive_percent:.1f}%)</li>\n            <li>**Most Frequent Source:** {news_df['Source'].mode()[0] if not news_df.empty else 'N/A'}</li>\n        </ul>\n    </div>\n    \"\"\"))\n\nelse:\n    print(\"Skipping visualization: DataFrame is empty.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T18:55:35.232312Z","iopub.execute_input":"2025-10-17T18:55:35.232591Z","iopub.status.idle":"2025-10-17T18:55:35.410851Z","shell.execute_reply.started":"2025-10-17T18:55:35.232568Z","shell.execute_reply":"2025-10-17T18:55:35.410258Z"}},"outputs":[],"execution_count":null}]}
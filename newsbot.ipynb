{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":121144,"databundleVersionId":14484960,"sourceType":"competition"},{"sourceId":208024,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":5388,"modelId":3533}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div style=\"\n  background: linear-gradient(160deg, #f4f8f7, #8cdbd8);\n  padding: 28px;\n  border-radius: 16px;\n  font-family: 'Inter', 'Roboto', 'Fira Sans', sans-serif;\n  color: #1b1f23;\n  line-height: 1.75;\n  max-width: 900px;\n  margin: 30px auto;\n  box-shadow: 0 6px 24px rgba(0,0,0,0.12);\n\">\n\n<h1 style=\"color:#03888f; text-align:center; margin-top:0;\">\nüì∞ NewsBot üóûÔ∏è: AI-Powered News Digest with Gemma\n</h1>\n\n<p style=\"font-size:1.1em; text-align:center;\">Welcome to the <strong>üì∞ NewsBot üóûÔ∏è</strong> Notebook!</p>\n\n<blockquote style=\"\n  border-left: 4px solid #00ffaa;\n  padding-left: 16px;\n  margin: 18px 0;\n  color:#2c3e50;\n  font-style:italic;\n  background-color: rgba(0, 255, 170, 0.05);\n\">\nIn today's fast-paced world, staying informed without being overwhelmed is a constant challenge. This notebook provides a hands-on solution by demonstrating how to build a fully automated system for generating a concise, insightful news digest.\n</blockquote>\n\n<p>We will harness the power of <b>Gemma</b> üß†, Google's family of lightweight, state-of-the-art open models, to perform sophisticated Natural Language Processing (NLP) tasks. The entire pipeline, from data fetching to final report, is handled right within this environment.</p>\n\n<hr style=\"border:none; border-top:2px solid #c0c6d1; margin:28px 0;\">\n\n\n<div style=\"\n    background: linear-gradient(145deg, #e0f7f7, #d0fff0);\n    border-radius: 16px;\n    padding: 24px 28px;\n    box-shadow: 0 8px 24px rgba(0,0,0,0.12);\n    max-width: 800px;\n    margin: 20px auto;\n    font-family: 'Inter', 'Roboto', 'Fira Sans', sans-serif;\n    color: #1b1f23;\n    line-height: 1.75;\n\">\n\n<h3 style=\"color:#1c9fa6; text-align:center;\">‚ú® üì∞ NewsBot üóûÔ∏è Capabilities & Highlights</h3>\n\n<ul style=\"color:#2c3e50; font-size:1.05em; line-height:1.8; padding-left:20px;\">\n  <li>‚öôÔ∏è <b>Automated News Digest Generation:</b> Instantly create and export polished HTML reports ready to share.</li>\n  <li>üìù <b>AI-Powered Summaries:</b> Gemma LLM transforms lengthy articles into concise, actionable insights.</li>\n  <li>üöÄ <b>Ethical News Dashboard:</b> Track and analyze trends responsibly with transparency at every step.</li>\n  <li>ü¶æ <b>Automated News Analysis:</b> From raw headlines to meaningful insights, NewsBot highlights what matters most.</li>\n  <li>üîÄ <b>Hybrid RAG & AI Analytics Engine:</b> Combines retrieval-augmented generation with advanced analytics for smarter intelligence.</li>\n</ul>\n\n</div>\n\n\n<hr style=\"border:none; border-top:2px solid #c0c6d1; margin:28px 0;\">\n\n<h3 style=\"color:#1c9fa6;\">üõ†Ô∏è Core Technologies of the First Section</h3>\n\n<ul style=\"color:#2c3e50;\">\n  <li><b>Data Source:</b> A dedicated <b>News API</b> connection üîó to fetch the latest English-language articles focused on \"Artificial Intelligence.\"</li>\n  <li><b>NLP Model:</b> <b>Gemma</b>, deployed via the <b>KerasNLP</b> library, for performing abstractive summarization.</li>\n  <li><b>Visualization:</b> The <b>WordCloud</b> library üñºÔ∏è to generate visual representations of key news trends.</li>\n  <li><b>Final Output:</b> A polished and shareable <b>HTML report</b> üìß.</li>\n</ul>\n\n<hr style=\"border:none; border-top:2px solid #c0c6d1; margin:28px 0;\">\n\n<h3 style=\"color:#1c9fa6;\">üöÄ The Digest Pipeline: From Raw Data to Final Report</h3>\n\n<ol style=\"color:#2c3e50;\">\n  <li><b>üåê Data Acquisition</b> ‚Äî Connect to the News API and pull fresh articles related to <b>AI</b>.</li>\n  <li><b>üß† Intelligent Summarization</b> ‚Äî Use the Gemma model to condense lengthy articles into clear, digestible summaries. Represented as: <code>S(A) = s</code></li>\n  <li><b>üìä Insight & Visualization</b> ‚Äî Analyze summaries to extract key themes and generate visuals such as a <b>Word Cloud</b>.</li>\n  <li><b>‚úÖ Report Generation</b> ‚Äî Compile all content into a single, polished <b>HTML news digest</b>.</li>\n</ol>\n\n<p style=\"text-align:center; font-weight:bold; color:#013c40;\">\nGet ready to transform your news consumption from a time-sink into a source of focused, <b>AI-driven insight</b>! üí°\n</p>\n\n</div>\n","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-16T05:32:04.048423Z","iopub.execute_input":"2025-11-16T05:32:04.048812Z","iopub.status.idle":"2025-11-16T05:32:04.070428Z","shell.execute_reply.started":"2025-11-16T05:32:04.048781Z","shell.execute_reply":"2025-11-16T05:32:04.069394Z"},"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q -U keras\n!pip install --upgrade keras-nlp\n!pip install --upgrade tensorflow","metadata":{"trusted":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2025-11-16T05:32:04.072256Z","iopub.execute_input":"2025-11-16T05:32:04.072586Z","iopub.status.idle":"2025-11-16T05:32:57.044349Z","shell.execute_reply.started":"2025-11-16T05:32:04.072560Z","shell.execute_reply":"2025-11-16T05:32:57.043222Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip uninstall -y tensorflow-text\n!pip install tensorflow-text","metadata":{"trusted":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2025-11-16T05:32:57.045897Z","iopub.execute_input":"2025-11-16T05:32:57.046334Z","iopub.status.idle":"2025-11-16T05:33:39.198219Z","shell.execute_reply.started":"2025-11-16T05:32:57.046284Z","shell.execute_reply":"2025-11-16T05:33:39.196804Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip uninstall -y tensorflow-text\n\n# Downgrade TensorFlow and its addons to 2.18.0\n!pip install --upgrade --force-reinstall \\\n    tensorflow==2.18.0 \\\n    tensorboard==2.18.0 \\\n    tensorflow-text==2.18.0","metadata":{"trusted":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2025-11-16T05:33:39.201361Z","iopub.execute_input":"2025-11-16T05:33:39.201735Z","iopub.status.idle":"2025-11-16T05:34:56.354349Z","shell.execute_reply.started":"2025-11-16T05:33:39.201688Z","shell.execute_reply":"2025-11-16T05:34:56.352989Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Downgrade or reinstall gymnasium to the exact version required\n!pip install --upgrade --force-reinstall gymnasium==0.29.0","metadata":{"trusted":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2025-11-16T05:34:56.356204Z","iopub.execute_input":"2025-11-16T05:34:56.356548Z","iopub.status.idle":"2025-11-16T05:35:06.188613Z","shell.execute_reply.started":"2025-11-16T05:34:56.356512Z","shell.execute_reply":"2025-11-16T05:35:06.187593Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"os.environ[\"KERAS_BACKEND\"] = \"jax\"  # Or \"tensorflow\" or \"torch\".\nos.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.00\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T05:35:06.190111Z","iopub.execute_input":"2025-11-16T05:35:06.190428Z","iopub.status.idle":"2025-11-16T05:35:06.196074Z","shell.execute_reply.started":"2025-11-16T05:35:06.190397Z","shell.execute_reply":"2025-11-16T05:35:06.195101Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import requests\nimport time","metadata":{"trusted":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2025-11-16T05:35:06.197210Z","iopub.execute_input":"2025-11-16T05:35:06.197825Z","iopub.status.idle":"2025-11-16T05:35:06.502839Z","shell.execute_reply.started":"2025-11-16T05:35:06.197787Z","shell.execute_reply":"2025-11-16T05:35:06.501905Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nNEWS_API_KEY = UserSecretsClient().get_secret(\"NEWS_API_KEY\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T05:35:06.503854Z","iopub.execute_input":"2025-11-16T05:35:06.504177Z","iopub.status.idle":"2025-11-16T05:35:06.554269Z","shell.execute_reply.started":"2025-11-16T05:35:06.504146Z","shell.execute_reply":"2025-11-16T05:35:06.553266Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 1: Define Endpoint\n# url = f\"https://newsapi.org/v2/top-headlines?country=us&apiKey={NEWS_API_KEY}\"\n# url = f\"https://newsapi.org/v2/top-headlines?country=us&category=technology&apiKey={NEWS_API_KEY}\"\nurl = f\"https://newsapi.org/v2/everything?q=AI&language=en&apiKey={NEWS_API_KEY}\"\n\ndef fetch_news():\n    resp = requests.get(url)\n    resp.raise_for_status()\n    data = resp.json()\n    print(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} ‚Äì fetched {len(data['articles'])} articles\")\n    return data\n\n# always fetch on notebook start\nnews_data = fetch_news()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T05:35:06.555334Z","iopub.execute_input":"2025-11-16T05:35:06.555701Z","iopub.status.idle":"2025-11-16T05:35:06.904884Z","shell.execute_reply.started":"2025-11-16T05:35:06.555629Z","shell.execute_reply":"2025-11-16T05:35:06.903971Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom IPython.display import display, HTML\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\n# Step 2: Fetch News Data\nresponse = requests.get(url)\nresponse.raise_for_status() # Good practice to check for errors\nnews_data = response.json()\n\n# Step 3: Process News Articles\narticles = news_data.get(\"articles\", [])\ndf = pd.DataFrame(articles)\n\n# Step 4: Clean, Select, and Format Columns\ndf_clean = df[[\"source\", \"title\", \"description\", \"url\", \"publishedAt\"]].copy()\n\n# Extract the source name from the nested dictionary\ndf_clean[\"source\"] = df_clean[\"source\"].apply(lambda x: x[\"name\"] if isinstance(x, dict) else x)\n\n# Convert the ISO date string to a human-readable datetime object\ndf_clean[\"publishedAt\"] = pd.to_datetime(df_clean[\"publishedAt\"]).dt.strftime('%Y-%m-%d %H:%M')\n\n# Rename columns for presentation\ndf_clean.columns = [\"Source\", \"Title\", \"Description\", \"URL\", \"Published At\"]\n\n# Step 5: Display Prettier Output (using display() and HTML styling)\ndisplay(HTML(\"\"\"\n    <p style=\"margin-top: 30px; font-size: 1.2em; font-weight: bold; color: #912358;\">\n        üì∞ Latest News Digest üì∞\n    </p>\n\"\"\"))\n# Use 'styler' to make the DataFrame look better in the notebook\ndisplay(\n    df_clean.head(10).style\n    .set_properties(**{'font-size': '10pt', 'border': '1px solid lightgrey'})\n    .set_table_styles([{'selector': 'th', 'props': [('background-color', '#f0f0f0')]}]),\n)\n\n# Optional: Save as CSV (if needed)\ndf_clean.to_csv(\"news_data.csv\", index=False)\n\n# Step 6: Visualization (Word Cloud of Titles)\ndisplay(HTML(\"\"\"\n    <p style=\"margin-top: 30px; font-size: 1.2em; font-weight: bold; color: #912358;\">\n        ‚òÅÔ∏è Article Title Word Cloud ‚òÅÔ∏è\n    </p>\n\"\"\"))\ntext = \" \".join(df_clean[\"Title\"].dropna())\nwordcloud = WordCloud(\n    width=1000, \n    height=500, \n    background_color=\"white\",\n    colormap=\"magma\", # Choose a nice color scheme\n    collocations=False # Helps with cleaner word separation\n).generate(text)\n\nplt.figure(figsize=(12, 6))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T05:35:06.909002Z","iopub.execute_input":"2025-11-16T05:35:06.909470Z","iopub.status.idle":"2025-11-16T05:35:08.800951Z","shell.execute_reply.started":"2025-11-16T05:35:06.909444Z","shell.execute_reply":"2025-11-16T05:35:08.799856Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ‚öôÔ∏è Generate and Export the News Digest (HTML File)\n\n> **To create the file `news_digest.html` in your notebook's output, run the cell below.**  \n> If you do not wish to generate or download the file, feel free to skip this cell.\n\nOnce the script completes, you'll find the file in your notebook's **file browser/output pane**.\n\n---\n\n### News Digest Generator\n\nThis script utilizes a pretrained **Gemma model** (powered by `keras_nlp`) to analyze the latest **AI-related news articles**. It extracts key insights such as:\n\n- **Key Trends** in AI\n- **Ethical Concerns** (e.g., job displacement, privacy issues)\n- **Future Implications** for AI\n\nThe script then generates a **Markdown-based digest** that includes:\n\n- A **word cloud** of news titles\n- Ethical concerns and trends\n- A summary of the most relevant articles\n\nFinally, the output is **converted into HTML** and saved as `news_digest.html`.\n\n---\n\n### Requirements\n\nTo run this script, you'll need the following Python libraries:\n\n- `keras_nlp`\n- `tensorflow`\n- `pandas`\n- `markdown`\n- `wordcloud`\n- `matplotlib`","metadata":{}},{"cell_type":"code","source":"#!/usr/bin/env python\n\"\"\"\nNews Digest Generator\n\nThis script loads a pretrained Gemma model (using keras_nlp) to analyze the latest AI-related news items.\nIt extracts key trends, ethical concerns, and future implications, then generates a Markdown-based digest\nthat includes a word cloud of news titles. The final output is converted into HTML and saved.\n\nRequirements:\n- `keras_nlp`, `tensorflow`, `pandas`, `markdown`, `wordcloud`, `matplotlib`\n\"\"\"\n\nfrom functools import lru_cache\nimport re\nimport keras\nimport keras_nlp\nimport tensorflow as tf\nfrom markdown import markdown\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nimport io\nimport base64\n\n# -----------------------------------------------------------------------------\n# IMPORTANT: Load your datasource and select the relevant columns.\n# You must define 'df' (for example via pd.read_csv(...)) before running the script.\n#\n# Step 4: Select Relevant Columns (using \"title\", \"description\", \"url\", \"publishedAt\")\ndf = df[[\"title\", \"description\", \"url\", \"publishedAt\"]]\n# -----------------------------------------------------------------------------\n\n# -----------------------------------------------------------------------------\n# 1. Load the Gemma Model\n# -----------------------------------------------------------------------------\nprecisions = [tf.bfloat16, None]\ngemma_lm = None\n\nfor precision in precisions:\n    try:\n        gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\n            \"gemma_instruct_2b_en\", jit_compile=False, dtype=precision\n        )\n        print(f\"‚úÖ Gemma model loaded successfully with {precision if precision else 'default'} precision.\")\n        break\n    except Exception as e:\n        print(f\"‚ùå Error loading Gemma model ({precision}): {e}\")\n\n# -----------------------------------------------------------------------------\n# 2. Define the AIAnalyzer Class\n# -----------------------------------------------------------------------------\nclass AIAnalyzer:\n    \"\"\"NewsBot: AI-powered analysis of AI-related news.\"\"\"\n    \n    def __init__(self, model, output_format=\"markdown\"):\n        self.model = model\n        self.output_format = output_format\n\n    @lru_cache(maxsize=50)\n    def analyze(self, news_item: str) -> dict:\n        \"\"\"\n        Generates an AI-powered analysis of a news item.\n        Expects news_item to be a string containing the title and description.\n        \"\"\"\n        prompt = f\"\"\"\nPlease analyze the following news item and generate an insightful breakdown of it. Do NOT repeat any part of this prompt or include extra instructions in your final response. Base your analysis solely on the information provided.\n\nNews item:\n{news_item}\n\nYour analysis MUST be exactly structured as follows, starting immediately with \"1. Key Trends\":\n1. Key Trends: Identify industry patterns, emerging technologies, risks, and opportunities evident in this news item.\n2. Implications: Discuss real-world applications and ethical concerns related to this news item.\n3. AI Insights: Provide an AI-driven perspective on what this news item means for the future.\n\nEnsure your response is well-organized, informative, and engaging.\n\"\"\"\n        try:\n            raw = self.model.generate(prompt, max_length=2048).strip()\n            response = (\n                raw[0] if isinstance(raw, (list, tuple))\n                else raw.numpy()[0].decode() if isinstance(raw, tf.Tensor)\n                else str(raw)\n            ).strip()\n        except Exception as e:\n            response = f\"‚ö†Ô∏è Error generating content: {e}\"\n\n        # --- Cleanup Routine ---\n\n        # 1. Trim any text preceding \"1. Key Trends:\" unconditionally.\n        marker_index = response.find(\"1. Key Trends:\")\n        if marker_index != -1:\n            response = response[marker_index:].strip()\n\n        # 2. Remove any trailing text starting from known prompt ending markers.\n        response = re.sub(r'\\n\\s*Ensure your response.*', '', response, flags=re.IGNORECASE)\n\n        # 3. Remove known unwanted instruction lines line-by-line.\n        unwanted_lines = {\n            \"1. Key Trends:\",\n            \"1. Key Trends: Identify industry patterns, emerging technologies, risks, and opportunities evident in this news item.\",\n            \"2. Implications: Discuss real-world applications and ethical concerns related to this news item.\",\n            \"3. AI Insights: Provide an AI-driven perspective on what this news item means for the future.\"\n        }\n        clean_lines = []\n        for line in response.splitlines():\n            if line.strip() not in unwanted_lines:\n                clean_lines.append(line)\n        response = \"\\n\".join(clean_lines).strip()\n\n        # 4. Reduce excessive newlines.\n        response = re.sub(r\"\\n{3,}\", \"\\n\\n\", response).strip()\n\n        # 5. If nothing remains (or if the cleaned response is empty), supply a fallback analysis.\n        if not response:\n            response = (\n                \"1. Key Trends: The news item demonstrates significant AI innovation with clear emerging trends that could reshape the industry. \"\n                \"2. Implications: It underscores both opportunities and critical risks, including ethical and practical challenges. \"\n                \"3. AI Insights: Overall, this development signals a transformative evolution in AI technology.\"\n            )\n\n        return {\"news_item\": news_item, \"response\": response}","metadata":{"trusted":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2025-11-16T05:35:08.801938Z","iopub.execute_input":"2025-11-16T05:35:08.802613Z","iopub.status.idle":"2025-11-16T05:36:42.076447Z","shell.execute_reply.started":"2025-11-16T05:35:08.802586Z","shell.execute_reply":"2025-11-16T05:36:42.075364Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -----------------------------------------------------------------------------\n# 3. Process the News Data\n# -----------------------------------------------------------------------------\n# Process the top 10 news items from the DataFrame.\nnews_df = df.head(10)[['title', 'description', 'url', 'publishedAt']]\n\nif gemma_lm is not None:\n    analyzer = AIAnalyzer(gemma_lm)\n    results = []\n    for _, row in news_df.iterrows():\n        title = row['title']\n        description = row['description']\n        # Construct the news item prompt using title and description.\n        news_item_prompt = f\"Title: {title}\\nDescription: {description}\"\n        analysis_result = analyzer.analyze(news_item_prompt)\n        analysis_result.update({\n            \"title\": title,\n            \"description\": description,\n            \"url\": row[\"url\"],\n            \"publishedAt\": row[\"publishedAt\"]\n        })\n        results.append(analysis_result)\nelse:\n    results = []\n    print(\"No model available; cannot generate news analysis.\")\n\n# -----------------------------------------------------------------------------\n# 4. Generate Word Cloud & Build Markdown Digest\n# -----------------------------------------------------------------------------\ntext = \" \".join(df[\"title\"].dropna())\nwordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate(text)\n\n# Save the word cloud image to a BytesIO object and encode as base64.\nbuf = io.BytesIO()\nplt.figure(figsize=(10, 5))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.tight_layout()\nplt.savefig(buf, format='png', bbox_inches='tight')\nplt.close()\n\nimage_base64 = base64.b64encode(buf.getvalue()).decode('utf-8')\nimage_html = f'<img src=\"data:image/png;base64,{image_base64}\" alt=\"Word Cloud\" style=\"max-width: 100%; height: auto;\">'\n\n# -----------------------------------------------------------------------------\n# 5. Build Markdown Output and Convert to HTML\n# -----------------------------------------------------------------------------\nmarkdown_output = \"## üåüüîé AI-Powered News Digest üî•\\n\\n\"\nmarkdown_output += image_html + \"\\n\\n\"\n\nfor i, rep in enumerate(results, start=1):\n    markdown_piece = f\"\"\"\n---\n## {i}. üì∞ **{rep['title']}**\n\n**Description:** <br>\n{rep['description']}\n\nüïí **Published At:** {rep['publishedAt']} <br>\nüîó **URL:** [{rep['url']}]({rep['url']}) <br>\n\n{rep['response']}\n\"\"\"\n    markdown_output += markdown_piece\n\nhtml_content = markdown(markdown_output)\n\nhtml_page = f\"\"\"<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <title>AI-Powered News Digest</title>\n    <style>\n        body {{\n            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n            line-height: 1.6;\n            padding: 20px;\n            max-width: 900px;\n            margin: auto;\n            background-color: #f9f9f9; /* Added for a light background */\n        }}\n        \n        h1 {{\n            color: #1f4d49;\n            text-align: center;\n            border-bottom: 2px solid #1f4d49; /* Updated for a stronger header line */\n            padding-bottom: 10px;\n        }}\n        \n        h2 {{\n            color: #2a647d; /* Changed header color */\n            border-bottom: 1px solid #ddd;\n            padding-bottom: 5px;\n            margin-top: 30px;\n        }}\n    \n        a {{\n            color: #007bff; /* Changed link color */\n            text-decoration: none;\n        }}\n        \n        a:hover {{\n            text-decoration: underline;\n        }}\n        \n        hr {{\n            margin: 40px 0;\n            border: 0;\n            border-top: 5px dotted #e0e0e0; /* Changed to a dotted separator */\n        }}\n        \n        img {{\n            display: block;\n            margin: 20px auto;\n            border: 1px solid #ddd; /* Added border and radius for images */\n            border-radius: 5px;\n        }}\n    </style>\n</head>\n<body>\n{html_content}\n</body>\n</html>\n\"\"\"\n\nwith open(\"news_digest.html\", \"w\", encoding=\"utf-8\") as f:\n    f.write(html_page)\n\n# 1. Print the success message\nprint(\"‚úÖ Page generated successfully! Open 'news_digest.html' in your browser to view your AI-powered news digest.\")\n\n# 2. Display the HTML content directly in the notebook output cell\nprint(\"\\n--- Displaying HTML Output Below ---\")\ndisplay(HTML(html_page))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T05:36:42.077731Z","iopub.execute_input":"2025-11-16T05:36:42.078883Z","iopub.status.idle":"2025-11-16T06:45:30.784767Z","shell.execute_reply.started":"2025-11-16T05:36:42.078852Z","shell.execute_reply":"2025-11-16T06:45:30.781157Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <span style=\"color:#1c9fa6;\">üìù AI News Summary with Gemma LLM</span>\n\n\n<div style=\"background-color: #F0F8FF; padding: 20px; border-radius: 10px; border: 1px solid #D0E6FA; font-family: Arial, sans-serif; line-height: 1.6; color: #333;\">\n\n<h2>üìù AI News Summary with Gemma LLM</h2>\n\n<p>Stay up to date with the most popular <strong>AI</strong> and <strong>LLM</strong> news (excluding anything related to <em>crypto</em>), automatically analyzed using a <strong>Gemma large language model</strong>.</p>\n\n<div style=\"background-color: #F0F8FF; padding: 24px; border-radius: 12px; border: 1px solid #D0E6FA; font-family: 'Segoe UI', Arial, sans-serif; line-height: 1.6; color: #333; max-width: 850px; margin: 20px auto; box-shadow: 0 3px 10px rgba(0,0,0,0.05);\">\n\n  <h2 style=\"color: #007acc; text-align: center; margin-bottom: 10px;\">üìù AI News Summary with Gemma LLM</h2>\n\n  <p style=\"text-align: center; font-size: 1.05em; color: #444;\">\n    Stay up to date with the most popular <strong>AI</strong> and <strong>LLM</strong> news (excluding anything related to \n    <em>crypto</em>), automatically analyzed using a <strong>Gemma large language model</strong>.\n  </p>\n\n  <hr style=\"border: none; height: 1px; background: linear-gradient(90deg, #D0E6FA, #007acc, #D0E6FA); margin: 20px 0;\">\n\n  <h3 style=\"color: #005f99;\">‚öôÔ∏è How It Works:</h3>\n  <ul style=\"margin-left: 20px;\">\n    <li>üîó <strong>Fetch News:</strong> Pulls articles from NewsAPI using keywords like <code>AI</code> or <code>LLM</code> (excluding <code>crypto</code>).</li>\n    <li>üß† <strong>Analyze with LLM:</strong> Summarizes using the Gemma model.</li>\n    <li>üìä <strong>Display Results:</strong> Presents data in a clean, styled table including key metadata.</li>\n  </ul>\n\n  <h3 style=\"color: #005f99;\">‚úÖ What You Get:</h3>\n  <p>A streamlined table featuring the <strong>Top 5 AI-related articles</strong> with:</p>\n  <ul style=\"margin-left: 20px;\">\n    <li>‚úçÔ∏è <strong>Title</strong></li>\n    <li>üì∞ <strong>LLM-generated Summary</strong></li>\n    <li>üè∑Ô∏è <strong>Source</strong></li>\n    <li>üìÖ <strong>Published Date</strong></li>\n    <li>üîó <strong>Direct Link to Full Article</strong></li>\n  </ul>\n\n</div>\n","metadata":{}},{"cell_type":"code","source":"# Constants\nNEWS_QUERY = \"(AI OR LLM) NOT crypto\"\nNEWS_SORT = \"popularity\"\nNEWS_LANG = \"en\"\nNEWS_PAGE_SIZE = 10\n\n# Your API key must be set as an environment variable or manually provided\nurl = (\n    f\"https://newsapi.org/v2/everything\"\n    f\"?q={NEWS_QUERY}&language={NEWS_LANG}&sortBy={NEWS_SORT}\"\n    f\"&pageSize={NEWS_PAGE_SIZE}&apiKey={NEWS_API_KEY}\"\n)\n\n# Analysis & Display Settings\nLLM_TOP_COUNT = 5\nDISPLAY_COUNT = 5\n\n# --- Gemma Analysis Function ---\ndef analyze_article_with_gemma(title, description):\n    \"\"\"\n    Generate a concise summary and sentiment using the Gemma LLM.\n    \"\"\"\n    if 'gemma_lm' not in globals() or gemma_lm is None:\n        return \"LLM Not Loaded: Please initialize 'gemma_lm'.\"\n\n    prompt = f\"\"\"\n    Analyze the following news article snippet and generate a summary \n    including its main topic and overall sentiment (positive, negative, or neutral).\n\n    Title: \"{title}\"\n    Snippet: \"{description}\"\n    \n    Summary:\n    \"\"\"\n\n    try:\n        output = gemma_lm.generate(prompt, max_length=384)\n        summary = output.split(\"Summary:\")[-1].strip().split('\\n')[0].replace('\"', '')\n        return summary\n    except Exception as e:\n        return f\"LLM Analysis Error: {e}\"\n\n# --- Fetch Articles ---\ndef fetch_articles():\n    try:\n        print(\"üì° Fetching news data...\")\n        response = requests.get(url)\n        response.raise_for_status()\n        return response.json().get('articles', [])\n    except requests.exceptions.RequestException as e:\n        print(f\"‚ùå Request Error: {e}\")\n        return []\n\n# --- Main Display Logic ---\ndef process_and_display_articles(articles):\n    if not articles:\n        print(\"No articles available to display.\")\n        return\n\n    print(f\"üåü Displaying Top {DISPLAY_COUNT} AI/LLM Articles with LLM Analysis üåü\\n\")\n\n    display_data = []\n\n    for i, article in enumerate(articles[:DISPLAY_COUNT]):\n        title = article.get('title', 'N/A')\n        description = article.get('description', 'No description available.')\n        source = article.get('source', {}).get('name', 'Unknown Source')\n        date = article.get('publishedAt', '')[:10] or 'N/A'\n        link = article.get('url', '#')\n\n        llm_summary = \"N/A\"\n        if i < LLM_TOP_COUNT:\n            print(f\"üß† Analyzing Article {i+1} with Gemma...\")\n            llm_summary = analyze_article_with_gemma(title, description)\n\n        display_data.append({\n            \"Rank\": i + 1,\n            \"LLM Analysis\": llm_summary,\n            \"Title\": title,\n            \"Source\": source,\n            \"Date\": date,\n            \"Link\": f'<a href=\"{link}\" target=\"_blank\">Read Article</a>'\n        })\n\n    df = pd.DataFrame(display_data)\n\n    styled_df = df.style.set_table_styles([\n        {'selector': 'th', 'props': [\n            ('background-color', '#1E88E5'),\n            ('color', '#fff'),\n            ('font-size', '14px'),\n            ('text-align', 'center'),\n            ('padding', '12px 8px'),\n            ('border', '1px solid #BBDEFB')\n        ]},\n        {'selector': 'td', 'props': [\n            ('font-family', 'Arial, sans-serif'),\n            ('padding', '10px 8px'),\n            ('border', '1px solid #E0E0E0'),\n            ('vertical-align', 'top')\n        ]},\n        {'selector': 'tr:nth-child(even)', 'props': [\n            ('background-color', '#F9FBFD')\n        ]},\n        {'selector': 'tr:hover', 'props': [\n            ('background-color', '#E3F2FD')\n        ]},\n        {'selector': 'table', 'props': [\n            ('border-collapse', 'collapse'),\n            ('width', '100%'),\n            ('box-shadow', '0 2px 5px rgba(0, 0, 0, 0.1)')\n        ]},\n        {'selector': 'a', 'props': [\n            ('color', '#1E88E5'),\n            ('text-decoration', 'none'),\n            ('font-weight', 'bold')\n        ]}\n    ]).set_properties(\n        subset=['Rank', 'Date', 'Link'], **{'text-align': 'center'}\n    ).set_properties(\n        subset=['LLM Analysis', 'Title'], **{'text-align': 'left', 'max-width': '400px'}\n    ).hide(axis='index').set_caption(\n        f\"üì∞ Top {DISPLAY_COUNT} AI Articles (Analyzed by Gemma LLM)\"\n    )\n\n    display(HTML(styled_df.to_html(escape=False)))\n\n# --- Run Pipeline ---\narticles = fetch_articles()\nprocess_and_display_articles(articles)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T06:45:30.790830Z","iopub.execute_input":"2025-11-16T06:45:30.791360Z","iopub.status.idle":"2025-11-16T06:55:42.353042Z","shell.execute_reply.started":"2025-11-16T06:45:30.791302Z","shell.execute_reply":"2025-11-16T06:55:42.351910Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <span style=\"color:#ffb300;\">üöÄ LLM Ethical News Dashboard Pipeline Summary</span>\n\n<div style=\"\n  background: linear-gradient(145deg, #fff9e6, #fff0f0);\n  border: 3px dashed #ffb347;\n  border-radius: 18px;\n  padding: 28px;\n  font-family: 'Poppins', 'Comic Neue', sans-serif;\n  color: #2b2b2b;\n  box-shadow: 0 6px 18px rgba(255, 180, 70, 0.2);\n  max-width: 900px;\n  margin: 20px auto;\n  line-height: 1.7;\n\">\n\n  <h2 style=\"color:#ff7b00; text-align:center;\">üöÄ LLM Ethical News Dashboard Pipeline Summary</h2>\n\n  <p style=\"font-size:1.1em; text-align:center; color:#444; margin-top:-10px;\">\n    <strong>A 3-Phase AI-Driven Data Pipeline</strong> for uncovering how the world is reporting on the ethics of Large Language Models.\n  </p>\n\n  <hr style=\"border: none; height: 2px; background: linear-gradient(90deg, #ffb347, #ff7b00, #ffb347); margin: 20px 0;\">\n\n  <p>\n    This document captures the execution of a <strong>three-phase data pipeline</strong> that explores how global media covers the ethical implications of LLMs.\n    The final outcome includes <strong>three dynamic, interactive visualizations</strong> built from real news data.\n  </p>\n\n  <hr style=\"border: none; border-top: 2px dashed #ffb347; margin: 25px 0;\">\n\n  <h3>üêç Phase 1: Data Acquisition (News API / Python)</h3>\n  <ul>\n    <li><strong>Action:</strong> Leveraged <code>requests</code> in Python to build a <strong>paginated fetch loop</strong> via the News API, capturing every relevant article from the past <strong>7 days</strong>.</li>\n    <li><strong>Query:</strong> Designed a nuanced search for both <strong>LLM topics</strong> and <strong>ethical themes</strong>:\n      <ul>\n        <li><strong>LLM Keywords:</strong> ‚Äúlarge language model‚Äù OR LLM OR ChatGPT OR Gemini</li>\n        <li><strong>Ethical Focus:</strong> accountability OR ‚Äúintellectual property‚Äù OR job displacement OR data privacy OR environmental impact</li>\n        <li><strong>Exclusions:</strong> <em>NOT</em> (bias OR hallucination)</li>\n      </ul>\n    </li>\n    <li><strong>Result:</strong> A rich set of <code>raw_articles</code> ‚Äî clean, structured dictionaries representing the full week‚Äôs ethical LLM news, primed for analysis.</li>\n  </ul>\n\n  <hr style=\"border: none; border-top: 2px dashed #ffb347; margin: 25px 0;\">\n\n  <h3>ü§ñ Phase 2: LLM Data Structuring (GemmaCausalLM)</h3>\n  <ul>\n    <li><strong>Goal:</strong> Transform raw text into structured, labeled data for visualization.</li>\n    <li><strong>Tool:</strong> Used the <strong>Keras-NLP</strong> model <strong>GemmaCausalLM</strong> (<code>gemma_lm</code>) for article-level classification.</li>\n    <li><strong>Process:</strong> A custom Python function runs each article through <code>gemma_lm</code> using a precision-tuned prompt that outputs strict JSON with:\n      <ul>\n        <li><code>application</code> ‚Üí Primary LLM domain (e.g., ‚ÄúHealthcare / Science‚Äù)</li>\n        <li><code>concerns</code> ‚Üí Ethical issues (e.g., [\"Data Privacy\", \"Accountability & IP\"])</li>\n        <li><code>entities</code> ‚Üí Mentioned organizations/products (e.g., [\"Google\", \"OpenAI\"])</li>\n      </ul>\n    </li>\n    <li><strong>Result:</strong> A structured <strong>Pandas DataFrame</strong> (<code>df</code>) with clean, categorized fields ‚Äî ready for aggregation.</li>\n  </ul>\n\n  <hr style=\"border: none; border-top: 2px dashed #ffb347; margin: 25px 0;\">\n\n  <h3>üìä Phase 3: Visualization (Pandas & Plotly)</h3>\n  <p>The processed data fuels a trio of <strong>interactive Plotly charts</strong> that reveal the evolving conversation around LLM ethics:</p>\n  <ol>\n    <li>üìà <strong>Line Chart (Trend Over Time):</strong> Tracks daily volume of articles per ethical concern ‚Äî showing which issues are gaining momentum.</li>\n    <li>ü•ß <strong>Pie Chart (Overall Distribution):</strong> Highlights the proportion of each ethical theme across the entire dataset.</li>\n    <li>üìä <strong>Stacked Bar Chart (Entity vs. Application):</strong> Connects the top 7 organizations to their most frequent LLM application domains.</li>\n  </ol>\n\n  <hr style=\"border: none; border-top: 2px dashed #ffb347; margin: 25px 0;\">\n\n  <p style=\"text-align:center; font-weight:bold; font-size:1.1em; margin-top:25px;\">\n    üåü <strong>Insight Delivered:</strong> A magazine-ready view of LLM ethics in the media ‚Äî curated by data, clarified by AI.\n  </p>\n\n</div>\n","metadata":{}},{"cell_type":"code","source":"# ============================\n# üì¶ Imports & Setup\n# ============================\nimport os, re, json, math, requests\nimport pandas as pd\nimport plotly.express as px\nfrom tqdm import tqdm\nfrom datetime import datetime, timedelta, timezone\n\nimport plotly.io as pio\npio.renderers.default = \"iframe\"   # often the most reliable in Kaggle\n\n\n# Make sure you set your News API key in Kaggle environment variables\n# NEWS_API_KEY = os.getenv(\"NEWS_API_KEY\")\nBASE_URL = \"https://newsapi.org/v2/everything\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T06:55:42.354308Z","iopub.execute_input":"2025-11-16T06:55:42.354566Z","iopub.status.idle":"2025-11-16T06:55:44.949416Z","shell.execute_reply.started":"2025-11-16T06:55:42.354546Z","shell.execute_reply":"2025-11-16T06:55:44.948445Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================\n# üêç Phase 1: Data Acquisition\n# ============================\n\ndef build_query():\n    must_llm = '(\"large language model\" OR LLM OR ChatGPT OR Gemini)'\n    must_ethics = '(accountability OR \"intellectual property\" OR \"job displacement\" OR \"data privacy\" OR \"environmental impact\")'\n    exclusions = 'NOT (bias OR hallucination)'\n    return f'{must_llm} AND {must_ethics} AND {exclusions}'\n\ndef iso_day_range(days=7):\n    now = datetime.now(timezone.utc)\n    from_date = (now - timedelta(days=days)).replace(microsecond=0)\n    return from_date.isoformat(), now.replace(microsecond=0).isoformat()\n\ndef fetch_all_articles(page_size=100, max_pages=None):\n    q = build_query()\n    from_iso, to_iso = iso_day_range(7)\n\n    params = {\n        \"q\": q,\n        \"from\": from_iso,\n        \"to\": to_iso,\n        \"language\": \"en\",\n        \"sortBy\": \"publishedAt\",\n        \"pageSize\": page_size,\n        \"apiKey\": NEWS_API_KEY,\n    }\n\n    resp = requests.get(BASE_URL, params=params, timeout=30)\n    resp.raise_for_status()\n    data = resp.json()\n\n    total = data.get(\"totalResults\", 0)\n    pages = math.ceil(total / page_size)\n    if max_pages is not None:\n        pages = min(pages, max_pages)\n\n    articles = data.get(\"articles\", [])\n    for page in tqdm(range(2, pages + 1), desc=\"Fetching pages\"):\n        params[\"page\"] = page\n        r = requests.get(BASE_URL, params=params, timeout=30)\n        r.raise_for_status()\n        j = r.json()\n        articles.extend(j.get(\"articles\", []))\n\n    seen, raw_articles = set(), []\n    for a in articles:\n        url = a.get(\"url\")\n        if not url or url in seen:\n            continue\n        seen.add(url)\n        raw_articles.append({\n            \"title\": a.get(\"title\") or \"\",\n            \"description\": a.get(\"description\") or \"\",\n            \"source\": (a.get(\"source\") or {}).get(\"name\") or \"\",\n            \"url\": url,\n            \"publishedAt\": a.get(\"publishedAt\") or \"\",\n        })\n    return raw_articles\n\n# Example run\n# raw_articles = fetch_all_articles(max_pages=2)\n# print(len(raw_articles))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T06:55:44.950759Z","iopub.execute_input":"2025-11-16T06:55:44.951012Z","iopub.status.idle":"2025-11-16T06:55:44.962246Z","shell.execute_reply.started":"2025-11-16T06:55:44.950993Z","shell.execute_reply":"2025-11-16T06:55:44.961146Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================\n# ü§ñ Phase 2: Classification\n# ============================\n\nCLOSED_APPLICATIONS = {\n    \"Healthcare/Science\",\"Education\",\"Finance\",\"Government/Public Sector\",\n    \"Enterprise/Work\",\"Consumer\",\"Media/Entertainment\",\"Research\",\"Legal\"\n}\nCLOSED_CONCERNS = {\"Data Privacy\",\"Accountability & IP\",\"Job Displacement\",\"Environmental Impact\",\"Other\"}\n\ndef run_gemma(prompt: str, max_len: int = 512) -> str:\n    \"\"\"Use the already-imported gemma_lm to generate text.\"\"\"\n    output = gemma_lm.generate(prompt, max_length=max_len)\n    if isinstance(output, dict) and \"text\" in output:\n        return output[\"text\"]\n    elif isinstance(output, list):\n        return output[0].get(\"text\", \"\")\n    return str(output)\n\ndef extract_json(text: str) -> dict:\n    match = re.search(r\"\\{.*\\}\", text, flags=re.DOTALL)\n    if not match:\n        return {}\n    try:\n        return json.loads(match.group(0))\n    except json.JSONDecodeError:\n        cleaned = re.sub(r\",\\s*}\", \"}\", match.group(0))\n        cleaned = re.sub(r\",\\s*]\", \"]\", cleaned)\n        try:\n            return json.loads(cleaned)\n        except:\n            return {}\n\ndef normalize_fields(obj: dict) -> dict:\n    app = obj.get(\"application\", \"\")\n    concerns = obj.get(\"concerns\", [])\n    entities = obj.get(\"entities\", [])\n\n    if app not in CLOSED_APPLICATIONS:\n        app = \"Other\"\n\n    if not isinstance(concerns, list):\n        concerns = [concerns]\n    concerns = [c for c in concerns if c in CLOSED_CONCERNS] or [\"Other\"]\n\n    if not isinstance(entities, list):\n        entities = [entities]\n    entities = [e for e in entities if isinstance(e, str) and e.strip()] or [\"Other\"]\n\n    return {\"application\": app, \"concerns\": concerns, \"entities\": entities}\n\ndef classify_articles(raw_articles):\n    rows = []\n    for a in tqdm(raw_articles, desc=\"Classifying\"):\n        prompt = f\"\"\"\nYou are classifying news articles about the ethical implications of Large Language Models.\n\nReturn ONLY a valid JSON object with keys: application, concerns, entities.\n\nRules:\n- application: choose ONE from {list(CLOSED_APPLICATIONS)}\n- concerns: choose ALL that apply from {list(CLOSED_CONCERNS)}\n- entities: list key organizations/products mentioned explicitly. If unknown, use \"Other\".\n\nTITLE: {a['title']}\nDESCRIPTION: {a['description']}\n\"\"\"\n        out = run_gemma(prompt)\n        obj = extract_json(out)\n        norm = normalize_fields(obj)\n        rows.append({\n            \"title\": a[\"title\"],\n            \"description\": a[\"description\"],\n            \"source\": a[\"source\"],\n            \"url\": a[\"url\"],\n            \"publishedAt\": a[\"publishedAt\"],\n            \"application\": norm[\"application\"],\n            \"concerns\": norm[\"concerns\"],\n            \"entities\": norm[\"entities\"],\n        })\n    df = pd.DataFrame(rows)\n    df = df.explode(\"concerns\").reset_index(drop=True)\n    df[\"date\"] = pd.to_datetime(df[\"publishedAt\"]).dt.date\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T06:55:44.963960Z","iopub.execute_input":"2025-11-16T06:55:44.964329Z","iopub.status.idle":"2025-11-16T06:55:44.999988Z","shell.execute_reply.started":"2025-11-16T06:55:44.964296Z","shell.execute_reply":"2025-11-16T06:55:44.998390Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================\n# üöÄ Run the Full Pipeline\n# ============================\n\n# 1. Acquire\nraw_articles = fetch_all_articles(max_pages=2)  # limit pages for demo\nprint(f\"Fetched {len(raw_articles)} articles\")\n\n# 2. Classify\ndf = classify_articles(raw_articles)\nprint(df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T06:55:45.001225Z","iopub.execute_input":"2025-11-16T06:55:45.001902Z","execution_failed":"2025-11-16T06:58:36.468Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set console options (this doesn't affect the HTML style)\npd.set_option('display.width', 1000)\n\nstyled_df = (\n    df.style\n    .set_table_styles([\n        # Header styling\n        {'selector': 'thead th', \n         'props': [\n             ('background-color', '#0e8cb5'), \n             ('color', 'white'), \n             ('font-weight', 'bold'),\n             ('font-size', '12px')\n         ]}, \n        \n        # General cell styling (for borders)\n        {'selector': 'th, td',\n         'props': [\n             ('border', '1px solid #ddd')  # <-- ADDED light gray borders\n         ]},\n        \n        # Alternating row colors\n        {'selector': 'tbody tr:nth-child(odd)', \n         'props': [\n             ('background-color', '#f2f2f2'),\n             ('font-size', '12px'), \n         ]}, \n        {'selector': 'tbody tr:nth-child(even)', \n         'props': [\n             ('background-color', 'white'), \n             ('font-size', '12px')\n         ]}\n    ])\n    .set_properties(\n        subset=['title'],\n        **{\n            'min-width': '200px',\n            'width': '200px',\n            'white-space': 'normal',    \n            'word-wrap': 'break-word',  \n            'font-size': '12px'\n        }\n    )\n    .set_properties(\n        subset=['description'],\n        **{\n            'min-width': '700px',\n            'width': '700px',\n            'white-space': 'normal',    \n            'word-wrap': 'break-word', \n            'font-size': '12px'\n        }\n    )\n    .set_table_styles(\n        [{'selector': 'table', \n          'props': [\n              ('table-layout', 'fixed'), \n              ('width', '100%'),\n              ('border-collapse', 'collapse') # <-- Makes borders look cleaner\n          ]}],\n        overwrite=False \n    )\n    .hide(axis=\"index\") \n)\n\nhtml_output = styled_df.to_html()\nwith open('my_styled_table_with_borders.html', 'w', encoding='utf-8') as f:\n    f.write(html_output)\n\nstyled_df","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-16T06:58:36.468Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"\n  background-color: #f9f9f9;\n  border-left: 4px solid #000000;\n  padding: 12px 16px;\n  border-radius: 8px;\n  max-width: 850px;\n  margin: 15px auto;\n  font-family: 'Inter', 'Roboto', 'Fira Sans', sans-serif;\n  color: #1a1a1a;\n\">\n  <strong>üìä Generate and Display the Charts</strong>\n</div>\n","metadata":{}},{"cell_type":"code","source":"# ============================\n# üìä Phase 3: Visualization\n# ============================\n\ndef plot_trend_over_time(df):\n    trend = (df.groupby([\"date\", \"concerns\"])\n               .size()\n               .reset_index(name=\"count\"))\n    fig = px.line(\n        trend,\n        x=\"date\",\n        y=\"count\",\n        color=\"concerns\",\n        markers=True,\n        title=\"üìà Daily News Volume by Ethical Concern\"\n    )\n    fig.show()\n\ndef plot_overall_distribution(df):\n    dist = df.groupby(\"concerns\").size().reset_index(name=\"count\")\n    fig = px.pie(\n        dist,\n        names=\"concerns\",\n        values=\"count\",\n        title=\"ü•ß Overall Distribution of Ethical Concerns\"\n    )\n    fig.update_traces(textposition='inside', textinfo='percent+label')\n    fig.show()\n\ndef plot_entities_vs_application(df, top_n=7):\n    df_ent = df.explode(\"entities\")\n    top_entities = df_ent[\"entities\"].value_counts().head(top_n).index.tolist()\n    filtered = df_ent[df_ent[\"entities\"].isin(top_entities)]\n    pivot = (filtered.groupby([\"entities\", \"application\"])\n             .size()\n             .reset_index(name=\"count\"))\n    fig = px.bar(\n        pivot,\n        x=\"entities\",\n        y=\"count\",\n        color=\"application\",\n        title=f\"üìä Top {top_n} Entities by Associated Application Areas\",\n        barmode=\"stack\"\n    )\n    fig.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-16T06:58:36.468Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_trend_over_time(df)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-16T06:58:36.469Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_overall_distribution(df)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-16T06:58:36.469Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_entities_vs_application(df)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-16T06:58:36.469Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ---- 1. Line chart: Daily Volume per Ethical Concern (Dark Theme) ----\ndef plot_trend_over_time(df):\n    # Ensure 'date' column is in datetime format\n    df['date'] = pd.to_datetime(df['publishedAt'], errors='coerce').dt.date\n    \n    # Group by date and concerns to get the count of articles per day\n    trend = df.groupby([\"date\", \"concerns\"]).size().reset_index(name=\"count\")\n    \n    # Creating the line plot for trend over time\n    fig1 = px.line(\n        trend,\n        x=\"date\",        # The x-axis represents the date\n        y=\"count\",       # The y-axis represents the article count\n        color=\"concerns\",# Different lines for each concern\n        markers=True,\n        title=\"üìà Daily News Volume by Ethical Concern\",  # Dynamic title\n        labels={\n            \"date\": \"Date\", \n            \"count\": \"Number of Articles\", \n            \"concerns\": \"Ethical Concern\"\n        }\n    )\n    \n    # Update layout for dark theme\n    fig1.update_layout(\n        template=\"plotly_dark\",  # Apply dark theme for the plot\n        xaxis_title=\"Date\", \n        yaxis_title=\"Number of Articles\", \n        title_x=0.5,  # Center the title\n        xaxis=dict(\n            tickangle=45,  # Rotate date labels for better readability\n            tickformat=\"%b %d, %Y\"  # Format date labels\n        ),\n        yaxis=dict(\n            range=[0, trend[\"count\"].max() * 1.1],  # Start y-axis from 0 and leave some space above the max\n        ),\n        plot_bgcolor=\"rgb(31, 31, 31)\",  # Dark background for the plot\n        paper_bgcolor=\"rgb(18, 18, 18)\",  # Dark background for the paper\n        font=dict(color=\"white\"),  # White text for all labels\n        title_font=dict(size=18, color=\"white\"),  # White title\n        legend_title_font=dict(color=\"white\"),\n        legend=dict(font=dict(color=\"white\")),  # White legend text\n    )\n    \n    return fig1\n\n# Plot the trend over time\nfig1 = plot_trend_over_time(df)\n\n# Show the plot\nfig1.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-16T06:58:36.469Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ---- 2. Overall distribution ----\ndef plot_overall_distribution(df):\n    # Grouping by ethical concerns and counting the number of occurrences\n    dist = df.groupby(\"concerns\").size().reset_index(name=\"count\")\n    \n    # Creating the pie chart for overall distribution\n    fig2 = px.pie(\n        dist,\n        names=\"concerns\",  # Categories for the pie chart\n        values=\"count\",    # Values for each category\n        title=\"Overall Distribution of Ethical Concerns\",  # Dynamic title\n        color=\"concerns\",  # Different colors for each concern\n        color_discrete_sequence=px.colors.qualitative.Set2  # Set2 color palette\n    )\n    \n    # Update the pie chart to display percentages and labels inside the chart\n    fig2.update_traces(textposition='inside', textinfo='percent+label')\n    fig2.update_layout(\n        template=\"plotly_dark\",  # Apply dark theme\n        title_x=0.5,  # Center the title\n        paper_bgcolor=\"#2a2a2a\",  # Paper background color\n    )\n    \n    return fig2\n\n# Plot the overall distribution\nfig2 = plot_overall_distribution(df)\npio.show(fig2)  # Use plotly.io.show() to ensure proper rendering in Kaggle","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-16T06:58:36.469Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ---- 3. Entities vs Application ----\ndef plot_entities_vs_application(df, top_n=7):\n    # Exploding the 'entities' column so that each entity gets its row\n    df_ent = df.explode(\"entities\")\n    \n    # Get the top_n most frequent entities\n    top_entities = df_ent[\"entities\"].value_counts().head(top_n).index.tolist()\n    \n    # Filter the dataframe for only the top entities\n    filtered = df_ent[df_ent[\"entities\"].isin(top_entities)]\n    \n    # Grouping by entities and their corresponding applications\n    pivot = filtered.groupby([\"entities\", \"application\"]).size().reset_index(name=\"count\")\n    \n    # Creating the stacked bar chart\n    fig3 = px.bar(\n        pivot,\n        x=\"entities\",         # Entities on the x-axis\n        y=\"count\",            # Article count on the y-axis\n        color=\"application\",  # Color by application area\n        barmode=\"stack\",      # Stack the bars\n        title=f\"Top {top_n} Entities by Associated Application Areas\",  # Dynamic title\n        labels={\n            \"entities\": \"Entity\", \n            \"count\": \"Article Count\", \n            \"application\": \"Application Area\"\n        },\n        color_discrete_sequence=px.colors.qualitative.Set1  # Use Set1 color palette\n    )\n    \n    fig3.update_layout(\n        template=\"plotly_dark\",  # Apply dark theme\n        xaxis_title=\"Entities\",  # Label for x-axis\n        yaxis_title=\"Article Count\",  # Label for y-axis\n        title_x=0.5,  # Center the title\n        paper_bgcolor=\"#2a2a2a\",  # Paper background color\n        plot_bgcolor=\"#1e1e1e\",  # Plot background color\n    )\n    \n    return fig3\n\n# Plot entities vs application\nfig3 = plot_entities_vs_application(df)\npio.show(fig3)  # Use plotly.io.show() to ensure proper rendering in Kaggle","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-16T06:58:36.469Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ü¶æ Automated News Analysis: From Headlines to Actionable Insights\n\n### üöÄ Introduction: Uncovering Trends with NewsAPI and Sentiment Analysis\n\nWelcome to this core section of the automated news analysis project! The goal here is to transform raw news headlines into **actionable intelligence** on any given topic.\n\n---\n\n#### **Pipeline Overview**\n\n1.  **Dynamic Data Retrieval (NewsAPI):**\n    * We use the **NewsAPI** with Python to dynamically fetch a high-volume stream of the latest global news articles related to a specified keyword (e.g., 'Artificial Intelligence', 'Climate Policy', 'Market Volatility').\n\n2.  **Quantifying the Mood (VADER Sentiment Analysis):**\n    * The VADER (Valence Aware Dictionary and sEntiment Reasoner) library is applied to the headline and article text. VADER is specifically attuned to social media and general text, making it highly effective for news analysis.\n    * It quantifies the prevailing media mood, assigning a **Compound Sentiment Score** (ranging from -1.0 for extremely negative to +1.0 for extremely positive) to each article.\n\n---\n\n#### **üéØ Deliverables: The Power of the Output**\n\nThe combination of data retrieval and sentiment scoring generates highly readable outputs:\n\n* **A Comprehensive DataFrame:** A clean, organized table showing the news source, headline, publish date, and the calculated sentiment scores (Positive, Negative, Neutral, and Compound).\n* **Intuitive Visualization:** A chart illustrating the sentiment distribution over time and across news sources.\n\nThis snapshot of media coverage helps us quickly identify:\n\n| Insight | Description |\n| :--- | :--- |\n| **Emerging Trends** | Spikes in coverage (volume) combined with highly positive sentiment. |\n| **Risk Areas** | Consistent, high-volume coverage with pronounced negative sentiment. |\n| **Market Excitement** | Sudden, strong positive sentiment correlated with a key event. |","metadata":{}},{"cell_type":"code","source":"# 1. Installation and Imports\n!pip install newsapi-python vaderSentiment","metadata":{"trusted":true,"_kg_hide-output":true,"execution":{"execution_failed":"2025-11-16T06:58:36.469Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom newsapi import NewsApiClient\nfrom kaggle_secrets import UserSecretsClient\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import display, HTML\n\n# --- üí° USER INPUT CONFIGURATION ---\n# Define the search term here.\nsearch_query = \"AI or artificial intelligence\"\n# Define the desired number of articles to fetch per page (max 100 on free tier)\nPAGE_SIZE = 100\n# Define the language (e.g., 'en', 'de', 'fr')\nLANGUAGE = 'en'\n# ------------------------------------\n\nprint(f\"Configuration set: Analyzing news for '{search_query}'...\")\n\n# 2. Secure API Client Initialization\nprint(\"Initializing NewsAPI Client...\")\ntry:\n    secret_client = UserSecretsClient()\n    api_key = secret_client.get_secret(\"NEWS_API_KEY\")\n    newsapi = NewsApiClient(api_key=api_key)\n    print(\"API Client successfully initialized.\")\nexcept Exception as e:\n    print(f\"Error accessing secret: {e}. Please ensure 'NEWS_API_KEY' is set in Kaggle Secrets.\")\n    # Use placeholder to allow subsequent cells to run, but with an empty article list\n    articles = []\n    newsapi = None","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-16T06:58:36.469Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 3. Fetch News Articles\narticles = []\nif newsapi:\n    print(f\"Fetching articles for query: '{search_query}'...\")\n    try:\n        all_articles = newsapi.get_everything(\n            q=search_query,\n            language=LANGUAGE,\n            sort_by='relevancy',\n            page_size=PAGE_SIZE\n        )\n        articles = all_articles['articles']\n        total_results = all_articles['totalResults']\n        print(f\"Successfully retrieved {len(articles)} articles (Total found: {total_results}).\")\n    except Exception as e:\n        print(f\"An error occurred during the API call: {e}\")\n        articles = []\n\nif not articles:\n    print(\"\\n‚ö†Ô∏è No articles retrieved. Check your API key and query.\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-16T06:58:36.469Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 4. Data Processing and DataFrame Creation\nif articles:\n    news_df = pd.DataFrame(articles)\n    \n    # Extract source name\n    news_df['source_name'] = news_df['source'].apply(lambda x: x['name'] if isinstance(x, dict) and 'name' in x else 'Unknown')\n    news_df.drop('source', axis=1, inplace=True) \n    \n    # Convert date to datetime object\n    news_df['publishedAt'] = pd.to_datetime(news_df['publishedAt'], utc=True)\n    \n    # Select and rename relevant columns\n    news_df = news_df[['publishedAt', 'source_name', 'title', 'description', 'url', 'content']]\n    news_df.rename(columns={'publishedAt': 'Date', 'source_name': 'Source', 'title': 'Title', 'description': 'Description', 'url': 'URL'}, inplace=True)\n    \n    print(\"DataFrame successfully created and cleaned.\")\n    print(f\"DataFrame Shape: {news_df.shape}\")\nelse:\n    print(\"Skipping DataFrame creation as no articles were found.\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-16T06:58:36.469Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 5. Sentiment Analysis (VADER)\nif 'news_df' in locals() and not news_df.empty:\n    analyzer = SentimentIntensityAnalyzer()\n    \n    def get_sentiment_score(text):\n        \"\"\"Analyzes sentiment for a given text and returns the VADER compound score.\"\"\"\n        if pd.isna(text) or text is None or str(text) == 'None':\n            return 0.0\n        # Use description for sentiment\n        return analyzer.polarity_scores(str(text))['compound']\n\n    def categorize_sentiment(score):\n        \"\"\"Categorizes the VADER compound score.\"\"\"\n        if score >= 0.05:\n            return 'Positive'\n        elif score <= -0.05:\n            return 'Negative'\n        else:\n            return 'Neutral'\n\n    # Apply sentiment analysis\n    news_df['Sentiment_Score'] = news_df['Description'].apply(get_sentiment_score)\n    news_df['Sentiment'] = news_df['Sentiment_Score'].apply(categorize_sentiment)\n    \n    print(\"Sentiment analysis complete.\")\nelse:\n    print(\"Skipping sentiment analysis.\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-16T06:58:36.470Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 6. Styled Data Preview for Readability\n\nif 'news_df' in locals() and not news_df.empty:\n    \n    # Define a helper function to color sentiment scores\n    def color_sentiment(val):\n        \"\"\"Applies a color gradient based on the sentiment score.\"\"\"\n        if val >= 0.05:\n            # Green for Positive\n            color = '#90EE90' \n        elif val <= -0.05:\n            # Light Red for Negative\n            color = '#F08080' \n        else:\n            # Neutral/Grey\n            color = '#D3D3D3'\n        return f'background-color: {color}'\n\n    # Prepare DataFrame for display: show only key columns\n    display_cols = ['Date', 'Source', 'Title', 'Sentiment', 'Sentiment_Score']\n    styled_df = news_df[display_cols].head(10).style \\\n        .set_caption(f\"Top 10 News Articles for: '{search_query.title()}'\") \\\n        .map(color_sentiment, subset=['Sentiment_Score']) \\\n        .background_gradient(cmap='viridis', subset=['Sentiment_Score']) \\\n        .format({'Date': lambda t: t.strftime('%Y-%m-%d %H:%M'), 'Sentiment_Score': \"{:.3f}\"}) \\\n        .set_properties(**{'font-size': '10pt', 'border-color': 'lightgrey'}) \\\n        .hide(axis='index') # Hides the default index column\n        \n    display(styled_df)\nelse:\n    print(\"Cannot display data: DataFrame is empty.\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-16T06:58:36.470Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 7. Visualization: Sentiment Distribution\n\nif 'news_df' in locals() and not news_df.empty:\n    plt.figure(figsize=(8, 5))\n    \n    # Define colors for the chart\n    color_map = {'Positive': '#4CAF50', 'Neutral': '#FFC107', 'Negative': '#F44336'}\n    \n    sns.countplot(x='Sentiment', \n                  data=news_df, \n                  order=['Positive', 'Neutral', 'Negative'], \n                  palette=color_map)\n    \n    plt.title(f'Sentiment Distribution for News on: \"{search_query.title()}\"', \n              fontsize=14, \n              fontweight='bold')\n    plt.xlabel('Sentiment Category', fontsize=12)\n    plt.ylabel('Number of Articles', fontsize=12)\n    plt.grid(axis='y', linestyle='--', alpha=0.7)\n    plt.show()\n    \n    # Summary box\n    positive_count = news_df[news_df['Sentiment'] == 'Positive'].shape[0]\n    total_count = news_df.shape[0]\n    positive_percent = (positive_count / total_count) * 100 if total_count > 0 else 0\n    \n    display(HTML(f\"\"\"\n    <div style=\"border: 1px solid #007BFF; padding: 10px; border-radius: 5px; background-color: #E6F0FF;\">\n        <p><strong>üìä Analysis Summary:</strong></p>\n        <ul>\n            <li><strong>Total Articles:</strong> {total_count}</li>\n            <li><strong>Positive Articles:</strong> {positive_count} ({positive_percent:.1f}%)</li>\n            <li><strong>Most Frequent Source:</strong> {news_df['Source'].mode()[0] if not news_df.empty else 'N/A'}</li>\n        </ul>\n    </div>\n    \"\"\"))\n\nelse:\n    print(\"Skipping visualization: DataFrame is empty.\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-16T06:58:36.470Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üîÄ NewsBot: Hybrid RAG Pipeline & AI News Analytics Engine\n\n<div style=\"\n  background: linear-gradient(145deg, #3f4e69, #1a1f27);\n  border: 1.5px solid #2f343e;\n  border-radius: 16px;\n  padding: 32px;\n  font-family: 'Inter', 'Segoe UI', 'Roboto', sans-serif;\n  color: #e6edf3;\n  box-shadow: 0 0 24px rgba(0, 255, 200, 0.12);\n  line-height: 1.7;\n\">\n\n<h2 style=\"color:#00e0ff; text-align:center; font-size:2em;\">\n  üîÄ <span style=\"color:#00e0ff;\">NewsBot:</span> \n  <span style=\"color:#00ffaa;\">Hybrid RAG Pipeline & AI News Analytics Engine</span>\n</h2>\n\n<p style=\"font-size:1.1em; color:#c9d1d9; text-align:center; margin-top:-10px;\">\n  An <b>AI-powered analyst</b> that combines <span style=\"color:#00ffaa;\">retrieval-augmented generation</span> and <span style=\"color:#58a6ff;\">trend vector analytics</span> to decode the evolving world of Artificial Intelligence.\n</p>\n\n<hr style=\"border:none; border-top:1px solid #2f343e; margin:22px 0;\">\n\n<p>\n  This section of <strong>NewsBot</strong> functions as an intelligent news analyst ‚Äî performing targeted <strong>Data Collection</strong> on AI developments, then applying a hybrid pipeline that powers two main capabilities:\n</p>\n\n<ol style=\"color:#e6edf3;\">\n  <li><strong>üß† Real-Time Q&A (RAG):</strong> Delivers <span style=\"color:#00ffaa;\">contextually grounded summaries</span> and concise answers to user questions using the most recent AI news as reference material.</li>\n  <li><strong>üìä Trend Analysis:</strong> Conducts <span style=\"color:#58a6ff;\">vector-space exploration</span> to identify emerging AI topics, rising companies, and notable sector shifts.</li>\n</ol>\n\n<hr style=\"border:none; border-top:1px solid #2f343e; margin:22px 0;\">\n\n<h3 style=\"color:#00e0ff; font-size:1.5em;\">\n  üß© 0. Environment Setup and Indexing\n</h3>\n\n<p style=\"color:#f7f9fa;\">\n  Before activating the pipeline, these setup steps ensure a clean, stable environment ready for high-performance <strong>vector operations</strong> and <strong>semantic search indexing</strong>.\n</p>\n\n<h4 style=\"color:#00ffaa;\"> ‚öôÔ∏è Step 0.1: Install Dependencies and Clean Environment</h4>\n\n</div>\n","metadata":{}},{"cell_type":"code","source":"!pip uninstall sentence-transformers transformers torch numpy safetensors -y\n!pip install --upgrade torch numpy safetensors\n!pip install --upgrade sentence-transformers","metadata":{"trusted":true,"_kg_hide-output":true,"execution":{"execution_failed":"2025-11-16T06:58:36.470Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !nvcc --version\n!pip uninstall torch -y\n!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121","metadata":{"trusted":true,"_kg_hide-output":true,"execution":{"execution_failed":"2025-11-16T06:58:36.470Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install faiss-cpu","metadata":{"trusted":true,"_kg_hide-output":true,"execution":{"execution_failed":"2025-11-16T06:58:36.470Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"\n  background: linear-gradient(145deg, #3f4e69, #1a1f27);\n  border: 1.5px solid #2f343e;\n  border-radius: 16px;\n  padding: 32px;\n  font-family: 'Inter', 'Segoe UI', 'Roboto', sans-serif;\n  color: #e6edf3;\n  line-height: 1.7;\n  max-width: 800px;\n  margin: 20px auto;\n  box-shadow: 0 0 24px rgba(0, 255, 200, 0.12);\n\">\n\n<h4><span style=\"color:#00ffaa;\">Step 0.2: Data Collection & Acquisition</span></h4>\n\n<p style=\"color:#f7f9fa;\">\nIn this phase, we fetch recent news articles relevant to <span style=\"color:#8cf01a;\">Artificial Intelligence</span> from the News API.  \nThis is the first step in acquiring the <b>raw data</b> needed for the RAG system.\n</p>\n\n</div>\n","metadata":{}},{"cell_type":"code","source":"import requests\nimport json\nimport os\nimport datetime\nimport time\nfrom typing import List, Dict, Any\n\n# --- CONFIGURATION ---\nNEWS_API_KEY = UserSecretsClient().get_secret(\"NEWS_API_KEY\")\n\n# Output file path\nOUTPUT_FILE = \"raw_ai_news.jsonl\"\n\n# Search parameters\nSEARCH_QUERY = 'artificial intelligence OR AI'\nLANGUAGE = 'en'\nSORT_BY = 'publishedAt'\nPAGE_SIZE = 100 \nMAX_PAGES = 1 \n\n# Calculate 'from' date (7 days ago)\nTODAY = datetime.date.today()\nSEVEN_DAYS_AGO = TODAY - datetime.timedelta(days=7)\nFROM_DATE = SEVEN_DAYS_AGO.isoformat()\n# ---------------------\n\n# NOTE: NEWS_API_KEY is assumed to be defined in the environment.\n\ndef check_api_key() -> bool:\n    \"\"\"Checks if the News API key is available.\"\"\"\n    # Assuming NEWS_API_KEY is defined globally or imported\n    if 'NEWS_API_KEY' not in globals() and not os.getenv('NEWS_API_KEY'):\n        print(\"‚ùå ERROR: NEWS_API_KEY environment variable is not set.\")\n        print(\"Please set the environment variable to your key from NewsAPI.org.\")\n        return False\n    return True\n\ndef fetch_news_data() -> List[Dict[str, Any]]:\n    \"\"\"Fetches news articles from the News API.\"\"\"\n    if not check_api_key():\n        return []\n\n    print(f\"üì° Fetching articles for query: '{SEARCH_QUERY}'...\")\n    print(f\"    Timeframe: from {FROM_DATE} to {TODAY.isoformat()}\")\n\n    all_articles: List[Dict[str, Any]] = []\n\n    for page in range(1, MAX_PAGES + 1):\n        url = \"https://newsapi.org/v2/everything\"\n        params = {\n            'q': SEARCH_QUERY,\n            'from': FROM_DATE,\n            'sortBy': SORT_BY,\n            'language': LANGUAGE,\n            'pageSize': PAGE_SIZE,\n            'page': page,\n            'apiKey': globals().get('NEWS_API_KEY', os.getenv('NEWS_API_KEY'))\n        }\n\n        try:\n            response = requests.get(url, params=params, timeout=15)\n            response.raise_for_status() \n            data = response.json()\n        except requests.exceptions.RequestException as e:\n            print(f\"‚ùå API Request Failed: {e}\")\n            return []\n\n        if data.get('status') == 'ok':\n            articles = data.get('articles', [])\n            print(f\"    -> Page {page}: Fetched {len(articles)} articles.\")\n            all_articles.extend(articles)\n\n            total_results = data.get('totalResults', 0)\n            if len(all_articles) >= total_results or page >= MAX_PAGES:\n                break\n                \n            time.sleep(1)  \n        else:\n            print(f\"‚ùå API returned an error: {data.get('message', 'Unknown error')}\")\n            break\n\n    print(f\"‚úÖ Total articles collected: {len(all_articles)}\")\n    return all_articles\n\ndef save_data_to_jsonl(articles: List[Dict[str, Any]]):\n    \"\"\"Cleans articles and saves them to a JSONL file, preserving 'source'.\"\"\"\n    if not articles:\n        print(\"‚ö†Ô∏è No articles to save.\")\n        return\n\n    # Filter out articles with missing essential fields\n    cleaned_articles = [\n        article for article in articles \n        if article.get('title') and article.get('content') and article.get('url')\n    ]\n\n    print(f\"üßπ Saving {len(cleaned_articles)} cleaned articles to '{OUTPUT_FILE}'...\")\n    \n    try:\n        with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n            for article in cleaned_articles:\n                # Removed 'article.pop('source', None)' to fix the missing source issue.\n                article.pop('author', None)\n                article.pop('description', None)\n                \n                # Save as a single JSON line\n                f.write(json.dumps(article) + '\\n')\n                \n        print(f\"‚úÖ Data successfully saved to '{OUTPUT_FILE}'.\")\n    except IOError as e:\n        print(f\"‚ùå Error writing to file: {e}\")\n\nif __name__ == \"__main__\":\n    start_time = time.time()\n    articles = fetch_news_data()\n    save_data_to_jsonl(articles)\n    print(f\"\\nTotal script execution time: {time.time() - start_time:.2f} seconds.\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-16T06:58:36.470Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"\n  background: linear-gradient(145deg, #3f4e69, #1a1f27);\n  border: 1px solid #30363d;\n  border-radius: 16px;\n  padding: 24px;\n  font-family: 'Inter', 'Roboto', 'Fira Sans', sans-serif;\n  color: #ffffff !important;\n  line-height: 1.7;\n  max-width: 800px;\n  margin: 20px auto;\n  box-shadow: 0 0 20px rgba(0, 255, 170, 0.15);\n\">\n\n<h4><span style=\"color:#00ffaa !important;\">Step 0.3: Create the FAISS Vector Index</span></h4>\n\n<p style=\"color:#ffffff !important;\">\nThis critical step reads the raw data, uses <span style=\"color:#00bfff !important;\">NumPy</span> to generate simulated vectors (a technique used to bypass deep learning dependency issues), and then employs the highly efficient <b>FAISS</b> library (Facebook AI Similarity Search) to build a fast and scalable vector search index.\n</p>\n\n<ul style=\"color:#ffffff !important;\">\n  <li><b>Vector Index File:</b> <span style=\"background-color: #00bfff; color: #1a1f27; padding: 3px 6px; border-radius: 4px; font-family: 'Courier New', Courier, monospace;\">ai_news.faiss</span> ‚Äî The optimized FAISS index for vector similarity search</li>\n  <li><b>Metadata File:</b> <span style=\"background-color: #00bfff; color: #1a1f27; padding: 3px 6px; border-radius: 4px; font-family: 'Courier New', Courier, monospace;\">ai_news_metadata.jsonl</span> ‚Äî Article details linked by the index ID</li>\n</ul>\n\n</div>\n","metadata":{}},{"cell_type":"code","source":"import json\nimport pandas as pd\nimport time\nimport os\nimport sys\nimport numpy as np\n# Added FAISS library for efficient vector indexing and search\nimport faiss\nfrom typing import List, Dict, Any\n\n# NOTE: SentenceTransformer and its underlying dependencies (like torch/transformers) \n# have been REMOVED due to the \"GenerationMixin\" import error.\n# We will use simulated vectors to ensure the script completes.\n\n# --- CONFIGURATION ---\nRAW_DATA_FILE = \"raw_ai_news.jsonl\"\nFAISS_INDEX_FILE = \"ai_news.faiss\" \nMETADATA_FILE = \"ai_news_metadata.jsonl\"\n\n# The original dimension is maintained for consistency with the intended model (MiniLM-L6-v2)\nVECTOR_DIMENSION = 384 \n# ---------------------\n\ndef load_raw_data(raw_data_file: str) -> pd.DataFrame | None:\n    \"\"\"\n    Loads raw news data from a (potentially broken) JSONL file \n    into a Pandas DataFrame.\n    \n    This function is designed to handle JSONL files where single JSON \n    objects might be split across multiple lines, which breaks\n    standard 'lines=True' parsers.\n    \"\"\"\n    if not os.path.exists(raw_data_file):\n        print(f\"‚ùå Error: Raw data file '{raw_data_file}' not found. Please run the data collection script first.\")\n        return None\n        \n    print(f\"üî¨ Loading and repairing data from '{raw_data_file}'...\")\n    \n    records = []\n    buffer = \"\"\n    try:\n        with open(raw_data_file, 'r', encoding='utf-8') as f:\n            for line in f:\n                # Append the new line, stripping only the right-side newline char\n                buffer += line.rstrip('\\n')\n                \n                try:\n                    # Try to parse the buffer as a complete JSON object\n                    record = json.loads(buffer)\n                    # If successful, add to list and reset buffer\n                    records.append(record)\n                    buffer = \"\"\n                except json.JSONDecodeError:\n                    # If it fails, the JSON object is not yet complete.\n                    # We continue, and the next line will be added to the buffer.\n                    \n                    # Safety check: if buffer gets huge, the file is likely\n                    # corrupted in a way we can't fix.\n                    if len(buffer) > 2_000_000: # 2MB limit for a single entry\n                        print(f\"‚ùå Error: A single JSON entry seems to be larger than 2MB or the file is severely corrupted.\")\n                        print(f\"Problematic buffer starts with: {buffer[:200]}...\")\n                        return None\n                    continue\n                    \n        # After the loop, check if there's anything left in the buffer\n        if buffer.strip():\n             print(f\"‚ö†Ô∏è Warning: File ended with an incomplete JSON object in the buffer.\")\n             print(f\"Buffer content: {buffer[:200]}...\")\n\n    except Exception as e:\n        print(f\"‚ùå Error reading or parsing the raw data file: {e}\")\n        return None\n\n    if not records:\n        print(\"‚ö†Ô∏è Raw data file loaded but is empty or no valid JSON objects were found.\")\n        return None\n\n    # Convert the list of dictionaries to a DataFrame\n    try:\n        df = pd.DataFrame(records)\n    except Exception as e:\n        print(f\"‚ùå Error converting parsed records to DataFrame: {e}\")\n        return None\n    \n    # Ensure all column names are lowercase for consistent access\n    df.columns = [col.lower() for col in df.columns] \n\n    if df.empty:\n        print(\"‚ö†Ô∏è DataFrame created but is empty. No documents to embed.\")\n        return None\n        \n    print(f\"‚úÖ Successfully loaded and parsed {len(df)} records.\")\n    return df\n\ndef generate_simulated_embeddings(texts: List[str], dim: int) -> np.ndarray:\n    \"\"\"\n    SIMULATED EMBEDDING FUNCTION.\n    \n    This function creates placeholder vectors when deep learning dependencies fail,\n    allowing the FAISS index to be built and the RAG pipeline to continue.\n    \"\"\"\n    print(\"‚ö†Ô∏è WARNING: Using simulated, randomized vectors to bypass dependency error.\")\n    np.random.seed(42) # for reproducible results\n    \n    # Create vectors where one dimension is based on the text length (for mild variation)\n    embeddings = []\n    for text in texts:\n        # Generate a random base vector\n        vector = np.random.rand(dim).astype('float32')\n        # Introduce a feature based on the text length\n        vector[0] = len(text) / 1000.0 \n        # Normalize the vector (essential for L2 distance to mimic cosine similarity)\n        vector = vector / np.linalg.norm(vector)\n        embeddings.append(vector)\n        \n    return np.array(embeddings)\n\n\ndef create_faiss_index_and_metadata(raw_data_file: str):\n    \"\"\"\n    Reads raw news data, generates simulated embeddings, builds a FAISS index, \n    and saves the index and metadata separately.\n    \"\"\"\n\n    # This will now use the new, robust loading function\n    df = load_raw_data(raw_data_file)\n    if df is None:\n        return\n\n    # 1. Data Cleaning and Preparation\n    required_cols = ['title', 'content', 'url']\n    if not all(col in df.columns for col in required_cols):\n        print(f\"‚ùå Data error: Missing one or more required columns ('title', 'content', 'url'). Found: {df.columns.tolist()}\")\n        return\n\n    print(f\"üöÄ Starting vector index creation...\")\n    start_time = time.time()\n    \n    # Prepare text by combining title and content for the embedding function\n    df['text_to_embed'] = df['title'].fillna('').astype(str) + \" [SEP] \" + df['content'].fillna('').astype(str)\n    \n    print(f\"Generating embeddings for {len(df)} documents...\")\n    text_list = df['text_to_embed'].tolist()\n    \n    # --- CORE CHANGE: Using simulated vectors instead of SentenceTransformer ---\n    embeddings = generate_simulated_embeddings(text_list, VECTOR_DIMENSION)\n    # --------------------------------------------------------------------------\n\n    \n    print(\"Embedding generation complete.\")\n\n    \n    # 2. Build the FAISS Index\n    print(f\"Building FAISS Index (IndexFlatL2) with dimension {VECTOR_DIMENSION}...\")\n    \n    # Use IndexFlatL2 for L2 distance (which works well with normalized embeddings)\n    index = faiss.IndexFlatL2(VECTOR_DIMENSION)\n    \n    # Ensure embeddings are float32, C-contiguous.\n    # This is a common requirement for FAISS.\n    embeddings_for_faiss = np.ascontiguousarray(embeddings, dtype='float32')\n    index.add(embeddings_for_faiss)\n    \n    print(f\"‚úÖ FAISS Index built successfully with {index.ntotal} vectors.\")\n\n    \n    # 3. Save the Index\n    try:\n        faiss.write_index(index, FAISS_INDEX_FILE)\n        print(f\"‚úÖ FAISS Index saved to '{FAISS_INDEX_FILE}'.\")\n    except Exception as e:\n        print(f\"‚ùå Failed to save FAISS index: {e}\")\n        return\n\n    # 4. Save Metadata\n    metadata_records: List[Dict[str, Any]] = []\n    for index, row in df.iterrows():\n        # Clean up source field which might be a dictionary or a string\n        source_data = row.get('source', {})\n        source_name = source_data.get('name') if isinstance(source_data, dict) else source_data\n        \n        metadata_records.append({\n            'doc_id': index, # Use the DataFrame index as the document ID\n            'title': row['title'],\n            'content': row['content'],\n            'source': source_name,\n            'url': row['url'],\n            'publishedat': row.get('publishedat')\n        })\n\n    try:\n        with open(METADATA_FILE, 'w', encoding='utf-8') as outfile:\n            for record in metadata_records:\n                outfile.write(json.dumps(record) + '\\n')\n        print(f\"‚úÖ Metadata saved to '{METADATA_FILE}'.\")\n    except IOError as e:\n        print(f\"‚ùå Error writing metadata file: {e}\")\n        return\n\n    end_time = time.time()\n    print(f\"\\n‚ú® FAISS Vectorization and Indexing Complete!\")\n    print(f\"Total time taken: {end_time - start_time:.2f} seconds.\")\n\n\nif __name__ == \"__main__\":\n    try:\n        # Load the model has been removed. We go directly to index creation.\n        create_faiss_index_and_metadata(RAW_DATA_FILE)\n        \n    except Exception as e:\n        # Catch any remaining errors during the process\n        print(f\"\\nFATAL ERROR during vector store creation. Details: {e}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-16T06:58:36.470Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"\n  background: linear-gradient(145deg, #3f4e69, #1a1f27);\n  border: 2px solid #1f2733;\n  border-radius: 18px;\n  padding: 36px;\n  font-family: 'Inter', 'Segoe UI', 'Roboto', sans-serif;\n  color: #e6edf3;\n  line-height: 1.8;\n  max-width: 850px;\n  margin: 25px auto; \n  box-shadow: 0 0 28px rgba(0, 255, 200, 0.2);\n\">\n\n<h3 style=\"color:#00e0ff; text-align:left; margin-top:0;\">\n1. üìä Data Collection & Analytics Engine\n</h3>\n\n<h4 style=\"color:#58a6ff; margin-top:12px;\">\nAutomated Trend Analysis (The 3 Core Reports)\n</h4>\n\n<p style=\"color:#c9d1d9;\">\nThis step executes all three core analytics visualizations to provide actionable insights: \n<span style=\"color:#00ffaa;\"><b>Time-Series Trends</b></span>, <span style=\"color:#58a6ff;\"><b>Source Popularity</b></span>, and <span style=\"color:#ff77aa;\"><b>Keyword Frequency</b></span>. Each graph is designed to highlight patterns in real-time news data, making trends immediately visible to analysts.\n</p>\n\n</div>","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport string\nimport os\nimport plotly.express as px\nfrom IPython.display import display, HTML\nfrom collections import Counter\nfrom typing import Any\nimport json\nfrom datetime import datetime, timedelta\nimport random\nimport plotly.io as pio\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\n\npio.renderers.default = \"iframe\"  # Reliable rendering in notebooks\n\n# -----------------------------\n# Config\n# -----------------------------\nRAW_DATA_FILE = \"raw_ai_news.jsonl\"\nREPORT_TOP_N = 10\n\nSTOPWORDS = set([\n    'the','a','an','or','of','in','is','it','to','for','with','on','as','by','at','from',\n    'but','that','this','are','was','were','has','have','had','will','be','been','being',\n    'new','latest','exclusive','article','report','news','update','major','large','global',\n    'next','generation','future','race','chip','m4','vs','ai','artificial','intelligence',\n    'ml','machine','learning','llm','model','models','tech','data','cloud','system','tool',\n    'and','says','after','about','what','amazon','know','its','service','outage','may','can',\n    'gets','make','just','get','up','down','how','why','who','when','where','should','could',\n    'would','do','does','did','use','using','used','like','via','we','you','they','our','their',\n    'your','my','his','her','them','us','me','one','two','three','four','five','six','seven',\n    'eight','nine','ten','first','second','third','now','today','week','month','year','ago',\n    'ceo','cto','vcs','invests','funding','round','investors','invest'\n])\n\n# -----------------------------\n# Helper Functions\n# -----------------------------\ndef create_mock_data(filename: str):\n    \"\"\"Creates a dummy JSONL file for testing if the original file is missing.\"\"\"\n    print(f\"üõ†Ô∏è File '{RAW_DATA_FILE}' not found. Creating mock data for demonstration.\")\n    mock_articles = []\n    sources = [\"TechCrunch\", \"Reuters\", \"The Verge\", \"NY Times\", \"Wired\", \"CNBC\"]\n    keywords = [\"Google\", \"Microsoft\", \"OpenAI\", \"Anthropic\", \"Regulation\", \"Gemma\", \"Sora\", \"Apple\"]\n\n    start_date = datetime.now() - timedelta(days=30)\n    for i in range(150):  # Generate 150 articles\n        date = start_date + timedelta(days=random.randint(0, 29))\n        source_name = random.choice(sources)\n        title_keywords = random.sample(keywords, k=random.randint(1, 3))\n        title = f\"{random.choice(['New','Breakthrough'])} {title_keywords[0]} releases {title_keywords[-1]} news - {date.strftime('%Y-%m-%d')}\"\n        mock_articles.append({\n            \"title\": title,\n            \"publishedAt\": date.isoformat(),\n            \"source\": {\"id\": source_name.lower().replace(\" \", \"-\"), \"name\": source_name},\n            \"url\": f\"http://example.com/{i}\"\n        })\n\n    with open(filename, 'w') as f:\n        for article in mock_articles:\n            f.write(json.dumps(article) + '\\n')\n    print(f\"‚úÖ Mock data created with {len(mock_articles)} articles.\")\n\ndef print_styled_header(title: str):\n    \"\"\"Display HTML section header.\"\"\"\n    display(HTML(f\"\"\"\n        <div style='margin-top:25px; margin-bottom: 10px; border-top: 1px dashed #ccc;'></div>\n        <h3 style='color:#2c3e50; border-bottom: 2px solid #3498db; padding-bottom: 5px; background-color: #ecf0f1; padding: 10px;'>\n            {title}\n        </h3>\n    \"\"\"))\n\n# -----------------------------\n# Load & Prepare Data\n# -----------------------------\nif not os.path.exists(RAW_DATA_FILE):\n    create_mock_data(RAW_DATA_FILE)\n\ntry:\n    df = pd.read_json(RAW_DATA_FILE, lines=True)\nexcept Exception as e:\n    print(f\"‚ùå Error loading data: {e}\")\n    df = pd.DataFrame()\n\nif df.empty:\n    print(\"‚ùå Loaded data is empty.\")\nelse:\n    df['date'] = pd.to_datetime(df['publishedAt'], errors='coerce').dt.date\n    def extract_source_name(source: Any) -> str:\n        if isinstance(source, dict):\n            return source.get('name', 'Unknown Source')\n        if pd.isna(source):\n            return 'Unknown Source'\n        return str(source)\n    df['source'] = df['source'].apply(extract_source_name) if 'source' in df.columns else 'Unknown Source'\n\n# -----------------------------\n# Generate Dashboard\n# -----------------------------\ndef generate_dashboard(df: pd.DataFrame):\n    \"\"\"Generate an interactive 3-panel dashboard with all reports in one view.\"\"\"\n    if df.empty:\n        print(\"‚ùå No data available to display.\")\n        return\n\n    # 1Ô∏è‚É£ Daily News Volume\n    daily_counts = df.groupby('date').size().reset_index(name='Articles')\n\n    # 2Ô∏è‚É£ Top News Sources\n    top_sources = df['source'].value_counts().nlargest(REPORT_TOP_N).reset_index()\n    top_sources.columns = ['Source', 'Articles']\n\n    # 3Ô∏è‚É£ Keyword Frequency\n    punctuation_table = str.maketrans('', '', string.punctuation.replace('-', ''))\n    all_words = []\n    for title in df['title'].dropna().astype(str):\n        clean_title = title.lower().translate(punctuation_table)\n        words = [w for w in clean_title.split() if w not in STOPWORDS and len(w) > 2]\n        all_words.extend(words)\n    word_counts = Counter(all_words)\n    top_keywords = word_counts.most_common(REPORT_TOP_N)\n    keyword_df = pd.DataFrame(top_keywords, columns=['Keyword', 'Frequency'])\n    keyword_df['Keyword'] = keyword_df['Keyword'].str.replace('_', ' ').str.capitalize()\n\n    # -----------------------------\n    # Create Subplots\n    # -----------------------------\n    fig = make_subplots(\n        rows=3, cols=1,\n        subplot_titles=(\"1Ô∏è‚É£ Daily News Volume\", \"2Ô∏è‚É£ Top News Sources\", \"3Ô∏è‚É£ Top Keywords in Headlines\"),\n        vertical_spacing=0.15\n    )\n\n    # Daily Volume Line Chart\n    fig.add_trace(\n        go.Scatter(\n            x=daily_counts['date'],\n            y=daily_counts['Articles'],\n            mode='lines+markers',\n            line=dict(color='#3498db'),\n            name='Daily Volume'\n        ),\n        row=1, col=1\n    )\n\n    # Top Sources Horizontal Bar\n    fig.add_trace(\n        go.Bar(\n            x=top_sources['Articles'],\n            y=top_sources['Source'],\n            orientation='h',\n            marker=dict(color=top_sources['Articles'], colorscale='Viridis'),\n            text=top_sources['Articles'],\n            textposition='auto',\n            name='Top Sources'\n        ),\n        row=2, col=1\n    )\n\n    # Keyword Frequency Horizontal Bar\n    fig.add_trace(\n        go.Bar(\n            x=keyword_df['Frequency'],\n            y=keyword_df['Keyword'],\n            orientation='h',\n            marker=dict(color=keyword_df['Frequency'], colorscale='Magma'),\n            text=keyword_df['Frequency'],\n            textposition='auto',\n            name='Top Keywords'\n        ),\n        row=3, col=1\n    )\n\n    # Layout adjustments\n    fig.update_layout(\n        height=1200,\n        showlegend=False,\n        title_text=\"üìä AI News Analytics Dashboard\",\n        template='plotly_white'\n    )\n\n    fig.update_yaxes(autorange=\"reversed\", row=2, col=1)  # Top sources horizontal bar\n    fig.update_yaxes(autorange=\"reversed\", row=3, col=1)  # Keyword frequency horizontal bar\n\n    fig.show(renderer=\"iframe\")\n\n# -----------------------------\n# Display Dashboard\n# -----------------------------\ngenerate_dashboard(df)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-16T06:58:36.470Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"\n  background: linear-gradient(145deg, #3f4e69, #1a1f27);\n  border: 2px solid #1f2733;\n  border-radius: 18px;\n  padding: 36px;\n  font-family: 'Inter', 'Segoe UI', 'Roboto', sans-serif;\n  color: #e6edf3;\n  line-height: 1.8;\n  max-width: 850px;\n  margin: 25px auto;\n  box-shadow: 0 0 28px rgba(0, 255, 200, 0.2);\n\">\n\n<h3 style=\"color:#00e0ff; text-align:left; margin-top:0;\">\n2. üí° The Core RAG Pipeline (Q&A Interface)\n</h3>\n\n<p style=\"color:#c9d1d9;\">\nThis cell contains the <b>complete enhanced RAG pipeline</b>, fully executable in your environment.  \nIt demonstrates a <span style=\"color:#58a6ff;\"><b>Retrieval-Augmented Generation (RAG)</b></span> workflow using a <span style=\"color:#00ffaa;\"><b>local Keras-NLP GemmaCausalLM model</b></span> for analytical synthesis.\n</p>\n\n<h4 style=\"color:#58a6ff; margin-top:12px;\">\nEnhancements Include:\n</h4>\n\n<ul style=\"color:#c9d1d9;\">\n  <li>üß† <span style=\"color:#00ffaa;\"><b>Extended, richer AI output</b></span> via tuned generation parameters</li>\n  <li>üß© <span style=\"color:#58a6ff;\"><b>HTML/CSS-styled analyst reports</b></span> with icons and sections</li>\n  <li>üí° <span style=\"color:#ff77aa;\"><b>Smart prompt suggestions</b></span> for continued exploration</li>\n  <li>‚öôÔ∏è <span style=\"color:#00ffaa;\"><b>Graceful handling</b></span> when local model or FAISS index are missing</li>\n</ul>\n\n</div>\n","metadata":{}},{"cell_type":"code","source":"import json\nimport numpy as np\nimport faiss\nimport os\nimport time\nfrom IPython.display import HTML, display, Markdown\nfrom typing import List, Dict, Any\nimport re\nimport pandas as pd # Added for the robust data loading function\n\n# --- LOCAL MODEL IMPORTS (Kept for completeness) ---\ntry:\n    import tensorflow as tf\n\n    # Enable GPU memory growth if GPUs exist\n    gpus = tf.config.experimental.list_physical_devices('GPU')\n    if gpus:\n        try:\n            for gpu in gpus:\n                tf.config.experimental.set_memory_growth(gpu, True)\n            print(f\"‚úÖ Enabled memory growth on {len(gpus)} GPU(s).\")\n        except RuntimeError as e:\n            print(f\"‚ö†Ô∏è Could not set memory growth: {e}\")\n    else:\n        print(\"‚ö†Ô∏è No GPUs detected, running on CPU.\")\n\n    import keras_nlp\n    KERAS_AVAILABLE = True\nexcept ImportError:\n    print(\"‚ùå Keras-NLP or TensorFlow not found. Generation will be simulated.\")\n    KERAS_AVAILABLE = False\n\n# =========================================================================\n# CONFIGURATION\n# =========================================================================\nRAW_DATA_FILE = \"raw_ai_news.jsonl\" # Added for consistency, though only used in the vectorization script\nFAISS_INDEX_FILE = \"ai_news.faiss\"\nMETADATA_FILE = \"ai_news_metadata.jsonl\"\nTOP_K = 3\nVECTOR_DIMENSION = 384\n\nFAISS_INDEX: faiss.Index | None = None\nMETADATA_BANK: List[Dict[str, Any]] = []\n\n# --- GEMMA MODEL INITIALIZATION ---\ngemma_lm = None\nif KERAS_AVAILABLE:\n    print(\"Attempting to initialize GemmaCausalLM with memory optimizations...\")\n    try:\n        gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\n            \"gemma_instruct_2b_en\",\n            jit_compile=False,\n            dtype=tf.float16\n        )\n        print(\"‚úÖ Gemma model loaded successfully with float16 precision.\")\n    except Exception as e_fp16:\n        # print(f\"‚ùå Error loading Gemma model with float16: {e_fp16}\")\n        try:\n            gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\n                \"gemma_instruct_2b_en\",\n                jit_compile=False,\n                dtype=None\n            )\n            print(\"‚úÖ Gemma model loaded successfully with default float32 precision.\")\n        except Exception as e_fp32:\n            # print(f\"‚ùå Error loading Gemma model with float32: {e_fp32}\")\n            print(\"üî¥ Failed to load Gemma model. Falling back to simulated embeddings.\")\n            gemma_lm = None\nelse:\n    gemma_lm = None\n\n# =========================================================================\n# DATA UTILITIES (From the data preparation script - REQUIRED TO RUN FIRST)\n# =========================================================================\n\ndef load_raw_data(raw_data_file: str) -> pd.DataFrame | None:\n    \"\"\"\n    Loads raw news data from a (potentially broken) JSONL file \n    into a Pandas DataFrame. (Fixes multi-line JSON objects).\n    \"\"\"\n    if not os.path.exists(raw_data_file):\n        print(f\"‚ùå Error: Raw data file '{raw_data_file}' not found.\")\n        return None\n        \n    print(f\"üî¨ Loading and repairing data from '{raw_data_file}'...\")\n    \n    records = []\n    buffer = \"\"\n    try:\n        with open(raw_data_file, 'r', encoding='utf-8') as f:\n            for line in f:\n                # Append the new line, stripping only the right-side newline char\n                buffer += line.rstrip('\\n')\n                \n                try:\n                    # Try to parse the buffer as a complete JSON object\n                    record = json.loads(buffer)\n                    # If successful, add to list and reset buffer\n                    records.append(record)\n                    buffer = \"\"\n                except json.JSONDecodeError:\n                    # If it fails, the JSON object is not yet complete.\n                    if len(buffer) > 2_000_000: # Safety break\n                        print(f\"‚ùå Error: A single JSON entry seems to be larger than 2MB or the file is severely corrupted.\")\n                        return None\n                    continue\n                    \n        if buffer.strip():\n             print(f\"‚ö†Ô∏è Warning: File ended with an incomplete JSON object in the buffer.\")\n\n    except Exception as e:\n        print(f\"‚ùå Error reading or parsing the raw data file: {e}\")\n        return None\n\n    if not records:\n        print(\"‚ö†Ô∏è Raw data file loaded but is empty or no valid JSON objects were found.\")\n        return None\n\n    try:\n        df = pd.DataFrame(records)\n    except Exception as e:\n        print(f\"‚ùå Error converting parsed records to DataFrame: {e}\")\n        return None\n    \n    df.columns = [col.lower() for col in df.columns] \n    print(f\"‚úÖ Successfully loaded and parsed {len(df)} records.\")\n    return df\n\ndef generate_simulated_embeddings(texts: List[str], dim: int) -> np.ndarray:\n    \"\"\"\n    SIMULATED EMBEDDING FUNCTION.\n    \"\"\"\n    np.random.seed(42) # for reproducible results\n    embeddings = []\n    for text in texts:\n        vector = np.random.rand(dim).astype('float32')\n        vector[0] = len(text) / 1000.0 \n        vector = vector / np.linalg.norm(vector)\n        embeddings.append(vector)\n    return np.array(embeddings)\n\ndef create_faiss_index_and_metadata(raw_data_file: str):\n    \"\"\"\n    Reads raw news data, generates simulated embeddings, builds a FAISS index, \n    and saves the index and metadata separately. (The original vectorizer script logic)\n    \"\"\"\n\n    df = load_raw_data(raw_data_file)\n    if df is None:\n        return\n\n    required_cols = ['title', 'content', 'url']\n    if not all(col in df.columns for col in required_cols):\n        print(\"‚ùå Data error: Missing one or more required columns ('title', 'content', 'url').\")\n        return\n\n    print(f\"üöÄ Starting vector index creation...\")\n    \n    df['text_to_embed'] = df['title'].fillna('').astype(str) + \" [SEP] \" + df['content'].fillna('').astype(str)\n    \n    print(f\"Generating simulated embeddings for {len(df)} documents...\")\n    text_list = df['text_to_embed'].tolist()\n    embeddings = generate_simulated_embeddings(text_list, VECTOR_DIMENSION)\n    \n    print(f\"Building FAISS Index (IndexFlatL2) with dimension {VECTOR_DIMENSION}...\")\n    index = faiss.IndexFlatL2(VECTOR_DIMENSION)\n    embeddings_for_faiss = np.ascontiguousarray(embeddings, dtype='float32')\n    index.add(embeddings_for_faiss)\n    print(f\"‚úÖ FAISS Index built successfully with {index.ntotal} vectors.\")\n\n    try:\n        faiss.write_index(index, FAISS_INDEX_FILE)\n        print(f\"‚úÖ FAISS Index saved to '{FAISS_INDEX_FILE}'.\")\n    except Exception as e:\n        print(f\"‚ùå Failed to save FAISS index: {e}\")\n        return\n\n    metadata_records: List[Dict[str, Any]] = []\n    for index, row in df.iterrows():\n        source_data = row.get('source', {})\n        source_name = source_data.get('name') if isinstance(source_data, dict) else source_data\n        \n        metadata_records.append({\n            'doc_id': index, \n            'title': row['title'],\n            'content': row['content'],\n            'source': source_name,\n            'url': row['url'],\n            'publishedat': row.get('publishedat')\n        })\n\n    try:\n        with open(METADATA_FILE, 'w', encoding='utf-8') as outfile:\n            for record in metadata_records:\n                outfile.write(json.dumps(record) + '\\n')\n        print(f\"‚úÖ Metadata saved to '{METADATA_FILE}'.\")\n    except IOError as e:\n        print(f\"‚ùå Error writing metadata file: {e}\")\n        return\n\n# =========================================================================\n# RAG CORE UTILITIES\n# =========================================================================\n\ndef generate_simulated_embedding(query: str) -> np.ndarray:\n    \"\"\"Generates a query vector for search when the model is missing.\"\"\"\n    np.random.seed(hash(query) % 100000)\n    vector = np.random.rand(VECTOR_DIMENSION).astype('float32')\n    vector[0] = len(query) / 100.0\n    vector = vector / np.linalg.norm(vector)\n    return vector.reshape(1, -1)\n\ndef load_vector_store():\n    \"\"\"Loads FAISS index and metadata into memory.\"\"\"\n    global FAISS_INDEX, METADATA_BANK\n    if not os.path.exists(FAISS_INDEX_FILE) or not os.path.exists(METADATA_FILE):\n        print(f\"‚ùå Required FAISS index or metadata files missing. Please run vectorization step first.\")\n        return\n    try:\n        FAISS_INDEX = faiss.read_index(FAISS_INDEX_FILE)\n        print(f\"‚úÖ Loaded FAISS Index with {FAISS_INDEX.ntotal} vectors.\")\n        with open(METADATA_FILE, 'r', encoding='utf-8') as f:\n            METADATA_BANK = [json.loads(line) for line in f]\n        print(f\"‚úÖ Loaded {len(METADATA_BANK)} metadata documents.\")\n    except Exception as e:\n        print(f\"‚ùå Error loading FAISS or metadata: {e}\")\n        FAISS_INDEX = None\n        METADATA_BANK = []\n\ndef get_query_embedding(query: str, force_doc_id: int | None = None) -> np.ndarray:\n    \"\"\"Gets the query embedding, using simulation if model or reconstruction fails.\"\"\"\n    # (Implementation for real embedding model omitted, using simulation only)\n    return generate_simulated_embedding(query)\n\ndef retrieve_context(query_embedding: np.ndarray, top_k: int = TOP_K) -> list:\n    \"\"\"Searches the FAISS index for relevant documents.\"\"\"\n    if FAISS_INDEX is None or not METADATA_BANK:\n        return []\n    \n    # Ensure query vector is float32 and contiguous for FAISS\n    query_vector_faiss = np.ascontiguousarray(query_embedding, dtype='float32')\n    \n    distances_sq, indices = FAISS_INDEX.search(query_vector_faiss, top_k)\n    context_chunks = []\n    for i in range(top_k):\n        doc_index = indices[0][i]\n        if doc_index < 0 or doc_index >= FAISS_INDEX.ntotal:\n            continue\n        metadata_item = METADATA_BANK[doc_index]\n        chunk = {\n            'content': metadata_item.get('content', 'Content missing.'),\n            'title': metadata_item.get('title', 'Title missing.'),\n            'url': metadata_item.get('url', '#'),\n            'distance_l2': distances_sq[0][i]\n        }\n        context_chunks.append(chunk)\n    return context_chunks\n\n# =========================================================================\n# PROMPT & GENERATION (UPDATED)\n# =========================================================================\n\n# Updated System Instruction for better model compliance and structure\nRAG_SYSTEM_INSTRUCTION = (\n    \"You are an expert AI Business and Technology Analyst. Analyze and summarize recent AI-related developments \"\n    \"from the provided CONTEXT. \"\n    \"Your response MUST be organized into three distinct sections, using the Markdown headers exactly as shown below. \"\n    \"Use bullet points (*) for all content within each section.\\n\\n\"\n    \"### Key Trends\\n\"\n    \"### Announcements and Innovations\\n\"\n    \"### Strategies\\n\"\n    \"If information for a section is not available in the context, write a single bullet point that says 'No relevant information found in the context.'.\"\n)\n\ndef format_prompt_and_generate(query: str, context_chunks: list) -> str:\n    \"\"\"Formats the prompt and generates the response using the Gemma model.\"\"\"\n    global gemma_lm\n    if not gemma_lm:\n        return \"ERROR: Gemma model is not loaded or available.\"\n\n    context_text = \"\"\n    for i, chunk in enumerate(context_chunks):\n        # Clean context formatting for the model\n        content_snippet = re.sub(r'\\s+', ' ', chunk['content']).strip()\n        context_text += f\"[SOURCE {i+1}] Title: {chunk['title']}\\nContent: {content_snippet}\\nLink: {chunk['url']}\\n\\n\"\n\n    if not context_chunks:\n        return \"No relevant context chunks found for generation.\"\n\n    full_prompt = (\n        f\"{RAG_SYSTEM_INSTRUCTION}\\n\\n\"\n        f\"--- CONTEXT ---\\n{context_text}\"\n        f\"--- QUERY ---\\n{query}\\n\"\n        f\"--- REPORT ---\\n\"\n    )\n\n    print(f\"[DEBUG] Prompt length: {len(full_prompt)} characters\")\n    try:\n        response = gemma_lm.generate(\n            full_prompt,\n            max_length=4096\n        )\n        # Extract only the generated part after the prompt\n        generated_text = response[len(full_prompt):].strip()\n\n        if not generated_text:\n            return \"Generated output is empty. Try adjusting generation parameters or check context.\"\n\n        print(f\"[DEBUG] Generation success. Output snippet:\\n{generated_text[:300]}\")\n        return generated_text\n    except Exception as e:\n        return f\"ERROR during generation: {e}\"\n\n# =========================================================================\n# POST-PROCESSING & FORMATTING (UPDATED & PRETTIFIED)\n# =========================================================================\n\ndef format_sections(raw_response: str) -> str:\n    \"\"\"\n    Parses the raw model output, cleans it up, and wraps it in a professional, \n    dark-mode HTML structure for a 'prettier' display.\n    \"\"\"\n    expected_sections = {\n        \"Key Trends\": [],\n        \"Announcements and Innovations\": [],\n        \"Strategies\": []\n    }\n    # Standardize section names for robust matching\n    section_map = {\n        \"KEY TRENDS\": \"Key Trends\",\n        \"ANNOUNCEMENTS AND INNOVATIONS\": \"Announcements and Innovations\",\n        \"STRATEGIES\": \"Strategies\"\n    }\n    \n    current_section = None\n    lines = raw_response.splitlines()\n\n    for line in lines:\n        line = line.strip()\n        if not line:\n            continue\n            \n        # Detect headers: Robustly look for any line starting with #, ##, ###, or **\n        section_match = re.match(r\"^(?:\\*{2}|#+)\\s*([^:\\n]+):?\", line, re.IGNORECASE)\n        if section_match:\n            sec_title_raw = section_match.group(1).strip().upper()\n            if sec_title_raw in section_map:\n                current_section = section_map[sec_title_raw]\n            else:\n                current_section = None\n            continue\n            \n        # If we are in a section, capture the content line\n        if current_section:\n            # Clean and standardize the line to be a simple bullet point\n            # Removes starting *, -, ‚Ä¢, or just bare text and prepends a clean bullet\n            clean_line = re.sub(r\"^\\s*[\\*-‚Ä¢]?\\s*\", \"\", line).strip()\n            if clean_line:\n                 expected_sections[current_section].append(clean_line)\n\n    # --- HTML Rendering ---\n    \n    if all(len(v) == 0 for v in expected_sections.values()):\n        return f\"<p>The model did not follow the required format. Raw output:</p><pre style='white-space: pre-wrap;'>{raw_response.strip()}</pre>\"\n\n    output_lines = []\n    \n    for section, content_list in expected_sections.items():\n        # Title Styling\n        output_lines.append(f\"<h3 style='color:#00e676; border-bottom: 2px solid #00c853; padding-bottom: 5px; margin-top: 20px;'>{section}</h3>\")\n        \n        if content_list:\n            output_lines.append(\"<ul style='list-style-type: none; padding-left: 20px;'>\")\n            for item in content_list:\n                # Content Styling: Add a professional icon/emoji for flair\n                output_lines.append(f\"<li style='margin-bottom: 10px;'>&bull; {item}</li>\")\n            output_lines.append(\"</ul>\")\n        else:\n            output_lines.append(\"<p style='color:#ffc107;'><i>No insights available in the context for this section.</i></p>\")\n            \n    # Wrap in a visually distinct container\n    html_output = (\n        \"<div style='background:#212121;color:#f5f5f5;padding:30px;border-radius:15px;max-width:900px;box-shadow: 0 4px 12px rgba(0,0,0,0.5);font-family: \\\"Segoe UI\\\", Tahoma, Geneva, Verdana, sans-serif; line-height: 1.6;'>\"\n        \"<h2 style='text-align: center; color: #4fc3f7; margin-bottom: 25px;'>AI Analyst RAG Report</h2>\"\n        + \"\\n\".join(output_lines) \n        + \"</div>\"\n    )\n    return html_output\n\n\n# =========================================================================\n# SUGGESTED NEXT PROMPTS\n# =========================================================================\n\ndef suggest_next_prompts(query: str) -> List[str]:\n    \"\"\"Provides follow-up questions.\"\"\"\n    return [\n        \"How does this development compare with last year‚Äôs AI milestones?\",\n        \"What are the ethical implications of these trends?\",\n        \"How might this impact enterprise AI adoption?\"\n    ]\n\n# =========================================================================\n# MAIN PIPELINE\n# =========================================================================\n\ndef run_rag_pipeline(query: str):\n    \"\"\"The main execution function for the RAG process.\"\"\"\n    if FAISS_INDEX is None:\n        # Check if the FAISS files exist, if not, try to create them first\n        display(Markdown(\"‚ö†Ô∏è RAG files not loaded. Attempting to build FAISS index now...\"))\n        create_faiss_index_and_metadata(RAW_DATA_FILE)\n        load_vector_store()\n        if FAISS_INDEX is None:\n            display(Markdown(\"‚ùå **FATAL:** Could not create or load RAG files. Cannot continue.\"))\n            return\n\n    display(Markdown(\"### ü§ñ Running Enhanced RAG Pipeline\"))\n    query_vector = get_query_embedding(query)\n    context_chunks = retrieve_context(query_vector)\n    \n    if not context_chunks:\n        display(Markdown(\"‚ö†Ô∏è *No relevant context found in FAISS index. Aborting generation.*\"))\n        return\n\n    # Debug output of context\n    display(Markdown(f\"**[DEBUG] Retrieved {len(context_chunks)} context chunks:**\"))\n    for i, c in enumerate(context_chunks):\n        display(Markdown(f\"- **Chunk {i+1} Title:** {c['title'][:60]}\"))\n        display(Markdown(f\"  \\nContent snippet: `{c['content'][:100]}...`\"))\n\n    raw_response = format_prompt_and_generate(query, context_chunks)\n\n    # Show raw text output immediately for visibility\n    display(Markdown(\"### --- AI Analyst Report (Plain Text Preview) ---\"))\n    display(Markdown(f\"```\\n{raw_response}\\n```\"))\n    display(Markdown(\"### --- End of Preview ---\"))\n\n\n    # Display nicely formatted dark mode styled HTML report\n    formatted_html = format_sections(raw_response)\n    display(HTML(formatted_html))\n\n    # Display suggested next prompts in styled box\n    prompts = suggest_next_prompts(query)\n    sug_html = (\n        \"<div style='margin-top:20px;background:#333;padding:16px;border-radius:8px;max-width:950px;color:#ccf;font-family: \\\"Segoe UI\\\", sans-serif;'>\"\n        \"<h3 style='color:#99ff99;'>üí° Suggested Next Prompts</h3><ul style='list-style-type: square; padding-left: 25px;'>\"\n        + \"\".join(f\"<li>{p}</li>\" for p in prompts)\n        + \"</ul></div>\"\n    )\n    display(HTML(sug_html))\n    display(Markdown(\"‚úÖ Done! (Scroll above for full report and suggestions.)\"))\n\n# =========================================================================\n# ENTRY POINT\n# =========================================================================\n\n# Uncomment the lines below to enable interactive input.\n\"\"\"\nif __name__ == \"__main__\":\n    # Load vector store upon script start\n    load_vector_store()\n\n    print(\"Welcome to the Local RAG Pipeline Demo!\")\n    user_query = input(\"Enter your AI-related question:\\n> \")\n    run_rag_pipeline(user_query)\n\"\"\"\n\n# Automatically use an example query if interactive input is disabled\nif __name__ == \"__main__\":\n    # Load vector store upon script start\n    load_vector_store()\n\n    display(Markdown(\"### Welcome to the Local RAG Pipeline Demo!\"))\n    \n    # Example query to automatically run\n    example_query = \"What are the recent trends and innovations in AI\"\n    display(Markdown(f\"**Using example query:** {example_query}\"))\n    \n    # Run the RAG pipeline with the example query\n    run_rag_pipeline(example_query)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T18:36:13.489103Z","iopub.execute_input":"2025-11-17T18:36:13.489416Z","iopub.status.idle":"2025-11-17T18:36:13.538208Z","shell.execute_reply.started":"2025-11-17T18:36:13.489395Z","shell.execute_reply":"2025-11-17T18:36:13.536931Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_37/1570863585.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mfaiss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'faiss'"],"ename":"ModuleNotFoundError","evalue":"No module named 'faiss'","output_type":"error"}],"execution_count":2},{"cell_type":"code","source":"import json\nimport numpy as np\nimport faiss\nimport os\nimport time\nfrom IPython.display import HTML, display, Markdown\nfrom typing import List, Dict, Any\nimport re\n\n# --- LOCAL MODEL IMPORTS (RESTORED) ---\ntry:\n    import tensorflow as tf\n    # ====== NEW: Enable GPU memory growth ======\n    gpus = tf.config.experimental.list_physical_devices('GPU')\n    if gpus:\n        try:\n            for gpu in gpus:\n                tf.config.experimental.set_memory_growth(gpu, True)\n            print(f\"‚úÖ Enabled memory growth on {len(gpus)} GPU(s).\")\n        except RuntimeError as e:\n            print(f\"‚ö†Ô∏è Could not set memory growth: {e}\")\n    else:\n        print(\"‚ö†Ô∏è No GPUs detected, running on CPU.\")\n    import keras_nlp\n    KERAS_AVAILABLE = True\nexcept ImportError:\n    print(\"‚ùå Keras-NLP or TensorFlow not found. Generation will be simulated.\")\n    KERAS_AVAILABLE = False\n\n# =========================================================================\n# 1. CONFIGURATION\n# =========================================================================\nFAISS_INDEX_FILE = \"ai_news.faiss\"\nMETADATA_FILE = \"ai_news_metadata.jsonl\"\nTOP_K = 3\nVECTOR_DIMENSION = 384\n\n# Global bank for FAISS index and metadata\nFAISS_INDEX: faiss.Index | None = None\nMETADATA_BANK: List[Dict[str, Any]] = []\n\n# --- GEMMA MODEL INITIALIZATION (UPDATED with float16 + fallback) ---\ngemma_lm = None\nif KERAS_AVAILABLE:\n    print(\"Attempting to initialize GemmaCausalLM with memory optimizations...\")\n    print(\"-\" * 40)\n    try:\n        gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\n            \"gemma_instruct_2b_en\", jit_compile=False, dtype=tf.float16\n        )\n        print(f\"‚úÖ Gemma model loaded successfully with float16 precision.\")\n    except Exception as e_fp16:\n        print(f\"‚ùå Error loading Gemma model with float16: {e_fp16}\")\n        try:\n            gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\n                \"gemma_instruct_2b_en\", jit_compile=False, dtype=None\n            )\n            print(f\"‚úÖ Gemma model loaded successfully with default float32 precision.\")\n        except Exception as e_fp32:\n            print(f\"‚ùå Error loading Gemma model with float32: {e_fp32}\")\n            print(\"üî¥ Failed to load Gemma model. Falling back to simulated embeddings.\")\n            gemma_lm = None\nelse:\n    gemma_lm = None\n\n# =========================================================================\n# Utility Functions for Simulation and Loading\n# =========================================================================\ndef generate_simulated_embedding(query: str) -> np.ndarray:\n    np.random.seed(hash(query) % 100000)\n    vector = np.random.rand(VECTOR_DIMENSION).astype('float32')\n    vector[0] = len(query) / 100.0\n    vector = vector / np.linalg.norm(vector)\n    return vector.reshape(1, -1)\n\ndef load_vector_store():\n    global FAISS_INDEX, METADATA_BANK\n    if not os.path.exists(FAISS_INDEX_FILE) or not os.path.exists(METADATA_FILE):\n        print(f\"‚ùå Error: Required files not found. Ensure FAISS index creation (Step 0.3) was run.\")\n        print(f\"Missing: {FAISS_INDEX_FILE} and/or {METADATA_FILE}\")\n        return\n    try:\n        FAISS_INDEX = faiss.read_index(FAISS_INDEX_FILE)\n        print(f\"‚úÖ Loaded FAISS Index with {FAISS_INDEX.ntotal} vectors.\")\n        with open(METADATA_FILE, 'r', encoding='utf-8') as f:\n            METADATA_BANK = [json.loads(line) for line in f]\n        print(f\"‚úÖ Loaded {len(METADATA_BANK)} documents from metadata file.\")\n    except Exception as e:\n        print(f\"‚ùå Error loading FAISS or metadata: {e}\")\n        FAISS_INDEX = None\n        METADATA_BANK = []\n\nload_vector_store()\n\n# =========================================================================\n# STEP 1: Transformation (Query -> Vector)\n# =========================================================================\ndef get_query_embedding(query: str, force_doc_id: int | None = None) -> np.ndarray:\n    if FAISS_INDEX is not None and force_doc_id is not None and 0 <= force_doc_id < FAISS_INDEX.ntotal:\n        print(f\"‚ö†Ô∏è CONTROLLED RETRIEVAL: Using vector reconstructed from FAISS for Doc ID {force_doc_id} as query.\")\n        try:\n            doc_vector = FAISS_INDEX.reconstruct(force_doc_id)\n            return doc_vector.reshape(1, -1)\n        except Exception as e:\n            print(f\"‚ùå Error reconstructing vector from FAISS: {e}. Falling back to simulated embedding.\")\n    print(\"‚ö†Ô∏è WARNING: Generating simulated embedding.\")\n    return generate_simulated_embedding(query)\n\n# =========================================================================\n# STEP 2: Retrieval (Vector -> Context)\n# =========================================================================\ndef retrieve_context(query_embedding: np.ndarray, top_k: int = TOP_K) -> list:\n    if FAISS_INDEX is None or not METADATA_BANK:\n        return []\n    distances_sq, indices = FAISS_INDEX.search(query_embedding, top_k)\n    context_chunks = []\n    for i in range(top_k):\n        doc_index = indices[0][i]\n        if doc_index >= FAISS_INDEX.ntotal or doc_index < 0:\n            continue\n        distance_l2 = distances_sq[0][i]\n        metadata_item = METADATA_BANK[doc_index]\n        chunk = {\n            'content': metadata_item.get('content', 'Content missing.'),\n            'title': metadata_item.get('title', 'Title missing.'),\n            'url': metadata_item.get('url', '#'),\n            'distance_l2': distance_l2\n        }\n        context_chunks.append(chunk)\n    return context_chunks\n\n# =========================================================================\n# STEP 3: Augmentation & Generation (Gemma Synthesis)\n# =========================================================================\nRAG_SYSTEM_INSTRUCTION = (\n    \"You are an expert AI Business and Technology Analyst. Analyze and summarize recent AI-related developments \"\n    \"from the provided CONTEXT. Structure your response with the following sections:\\n\\n\"\n    \"**Key Trends:**\\n\"\n    \"* Bullet points describing notable AI trends or patterns\\n\\n\"\n    \"**Announcements and Innovations:**\\n\"\n    \"* Bullet points about product releases, research, or major news\\n\\n\"\n    \"**Strategies:**\\n\"\n    \"* Bullet points about company AI strategies or recommendations\\n\\n\"\n    \"Respond ONLY using the context provided. If no AI content is found, state that clearly.\\n\\n\"\n    \"üí° Suggested Next Prompts:\\n\"\n    \"- How does this development compare with last year‚Äôs AI milestones?\\n\"\n    \"- What are the ethical implications of these trends?\\n\"\n    \"- How might this impact enterprise AI adoption?\\n\\n\"\n    \"Use bullet points (-) for all content within each section.\"\n)\n\ndef format_prompt_and_generate(query: str, context_chunks: list) -> str:\n    global gemma_lm\n    if not gemma_lm:\n        return \"ERROR: Gemma model failed to load due to resource constraints. Cannot generate response.\"\n    \n    # Format Context Text with clear citations and separation for better model understanding\n    context_text = \"\"\n    for i, chunk in enumerate(context_chunks):\n        context_text += f\"[SOURCE {i+1}] Title: {chunk['title']}\\n\"\n        context_text += f\"Content: {chunk['content']}\\n\"\n        context_text += f\"Link: {chunk['url']}\\n\\n\"\n\n    if not context_chunks:\n        return \"The FAISS index returned no context chunks. Cannot generate a grounded report.\"\n\n    # Add explicit instructions to generate detailed, structured and longer outputs\n    detailed_instruction = (\n        \"\\nPlease provide a comprehensive, detailed analysis in each section, \"\n        \"with clear bullet points and examples where relevant. Use professional language \"\n        \"and avoid repeating the prompt or context.\\n\"\n    )\n\n    full_prompt = (\n        f\"{RAG_SYSTEM_INSTRUCTION}\"\n        f\"{detailed_instruction}\\n\\n\"\n        \"--- CONTEXT FOR AI ANALYST ---\\n\"\n        f\"{context_text}\"\n        \"\\n--- USER QUERY ---\\n\"\n        f\"{query}\\n\"\n        \"\\n--- ANALYST REPORT ---\"\n    )\n\n    print(\"\\n--- Sending Augmented Prompt to Local Gemma Model ---\")\n\n    try:\n        # Increase max_length to allow longer output, but watch memory limits\n        response = gemma_lm.generate(\n            full_prompt,\n            max_length=4096\n        )\n        # Extract generated text by removing the prompt prefix\n        final_answer = response[len(full_prompt):].strip()\n        if not final_answer:\n            return \"Gemma generated an empty response or repeated the prompt. Check model configuration or context quality.\"\n        return final_answer\n    except Exception as e:\n        return f\"ERROR during Gemma generation: {e}\"\n\n# =========================================================================\n# POST-PROCESSING FUNCTION TO FORMAT SECTIONS\n# =========================================================================\ndef format_sections(raw_response: str) -> str:\n    expected_sections = {\n        \"Key Trends\": [],\n        \"Announcements and Innovations\": [],\n        \"Strategies\": []\n    }\n    current_section = None\n    lines = raw_response.splitlines()\n    for line in lines:\n        line = line.strip()\n        if not line:\n            continue\n        section_match = re.match(r\"\\*{2}(.*)\\*{2}:?\", line)\n        if section_match:\n            sec_title = section_match.group(1).strip()\n            if sec_title in expected_sections:\n                current_section = sec_title\n            else:\n                current_section = None\n            continue\n        if current_section:\n            # Normalize bullet points for consistent formatting\n            if line.startswith(\"*\"):\n                expected_sections[current_section].append(line)\n            else:\n                expected_sections[current_section].append(f\"* {line}\")\n    \n    # Fallback to raw response if no structured info found\n    if all(len(v) == 0 for v in expected_sections.values()):\n        return raw_response.strip()\n\n    output_lines = []\n    for section, bullets in expected_sections.items():\n        output_lines.append(f\"**{section}:**\\n\")\n        if bullets:\n            output_lines.extend(bullets)\n        else:\n            output_lines.append(\"* No information available.\")\n        output_lines.append(\"\")  # blank line after section\n    return \"\\n\".join(output_lines).strip()\n\n# =========================================================================\n# MAIN FUNCTION TO RUN THE PIPELINE\n# =========================================================================\ndef run_rag_pipeline(query: str):\n    display(Markdown(\"### ü§ñ Running RAG Pipeline (Local Gemma)\"))\n    display(Markdown(f\"**[1/4] Transforming query:** `{query}`\"))\n    \n    query_vector = get_query_embedding(query)\n    \n    display(Markdown(\"**[2/4] Retrieving relevant context from FAISS index...**\"))\n    context_chunks = retrieve_context(query_vector)\n    if not context_chunks:\n        display(Markdown(\"‚ö†Ô∏è *No relevant context found in FAISS index. Aborting generation.*\"))\n        return\n    \n    display(Markdown(\"**[3/4] Generating response using local Gemma model...**\"))\n    raw_response = format_prompt_and_generate(query, context_chunks)\n    \n    display(Markdown(\"### --- Raw Gemma Response ---\"))\n    display(Markdown(raw_response))\n    \n    display(Markdown(\"**[4/4] Post-processing and formatting final report...**\"))\n    formatted_report = format_sections(raw_response)\n    \n    display(Markdown(\"### --- Final Analyst Report ---\"))\n    display(Markdown(formatted_report))\n\n# =========================================================================\n# SCRIPT ENTRY POINT\n# =========================================================================\n\n# Uncomment the lines below to enable interactive input.\n\"\"\"\nif __name__ == \"__main__\":\n    # Load vector store upon script start\n    load_vector_store()\n\n    print(\"Welcome to the Local RAG Pipeline Demo!\")\n    user_query = input(\"Enter your AI-related question:\\n> \")\n    run_rag_pipeline(user_query)\n\"\"\"\n\n# Automatically use an example query if interactive input is disabled\nif __name__ == \"__main__\":\n    # Load vector store upon script start\n    load_vector_store()\n\n    display(Markdown(\"### Welcome to the Local RAG Pipeline Demo!\"))\n    \n    # Example query to automatically run\n    example_query = \"What are the latest advancements in AI models?\"\n    display(Markdown(f\"**Using example query:** {example_query}\"))\n    \n    # Run the RAG pipeline with the example query\n    result = run_rag_pipeline(example_query)\n\n    # Show result directly\n    if isinstance(result, str):\n        display(Markdown(result))\n    else:\n        display(Markdown(f\"```json\\n{result}\\n```\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T18:33:34.569384Z","iopub.execute_input":"2025-11-17T18:33:34.569701Z","iopub.status.idle":"2025-11-17T18:33:34.680997Z","shell.execute_reply.started":"2025-11-17T18:33:34.569677Z","shell.execute_reply":"2025-11-17T18:33:34.679383Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_37/1712334220.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mfaiss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'faiss'"],"ename":"ModuleNotFoundError","evalue":"No module named 'faiss'","output_type":"error"}],"execution_count":1},{"cell_type":"markdown","source":"<h2>‚ú® Sessions &amp; State Management</h2>\n\n<div style=\"background-color:#f0f4ff; padding: 22px; border-radius: 14px; border: 1px solid #d9e4ff;\">\n\n  <h2>‚ú® Sessions &amp; State Management</h2>\n\n  <p>\n    State management is handled through the <b>InMemorySessionService</b>, enabling the agent system to persist data \n    and intermediate results across multi-step operations. Essential for \n    <b>robust, stateful, multi-agent workflows</b>.\n  </p>\n\n  <hr>\n\n  <h3>üîß How This Fulfills the Requirement</h3>\n\n  <h4>1. üóÇÔ∏è State Persistence</h4>\n  <p>\n    Step 1: News Fetcher retrieves raw articles from NewsAPI \n    (<code>raw_articles_list</code>) and stores them in the session for global access.\n  </p>\n\n  <h4>2. üîÑ State Retrieval</h4>\n  <p>\n    Step 2: Gemma LLM News Analyzer retrieves <code>raw_articles_list</code> from the session‚Äîno direct variable passing‚Äîdemonstrating \n    <b>decoupled, memory-backed communication</b>.\n  </p>\n\n  <h4>3. üì¶ Output Storage for Later Agents</h4>\n  <p>\n    Final AI-generated summaries (<code>final_summaries</code>) produced by Gemma are stored back. Downstream agents can use results without \n    re-running API calls or LLM analysis.\n  </p>\n\n  <hr>\n\n  <h3>üìã Verification from Execution Output</h3>\n\n  <table style=\"width:100%; border-collapse: collapse;\">\n    <thead>\n      <tr>\n        <th style=\"text-align:left; border-bottom: 1px solid #ccc; padding: 6px;\">Action</th>\n        <th style=\"text-align:left; border-bottom: 1px solid #ccc; padding: 6px;\">Log Entry</th>\n      </tr>\n    </thead>\n    <tbody>\n      <tr>\n        <td style=\"padding: 6px;\">üîê Storage (Step 1)</td>\n        <td style=\"padding: 6px;\">‚úî State 'raw_articles_list' stored.</td>\n      </tr>\n      <tr>\n        <td style=\"padding: 6px;\">üì• Retrieval (Step 2)</td>\n        <td style=\"padding: 6px;\">üì• Retrieved state 'raw_articles_list'.</td>\n      </tr>\n      <tr>\n        <td style=\"padding: 6px;\">ü§ñ Gemma Step</td>\n        <td style=\"padding: 6px;\">ü§ñ Gemma summarized article X: &lt;title&gt;</td>\n      </tr>\n      <tr>\n        <td style=\"padding: 6px;\">üì¶ Final Storage</td>\n        <td style=\"padding: 6px;\">‚úî State 'final_summaries' stored.</td>\n      </tr>\n      <tr>\n        <td style=\"padding: 6px;\">‚úÖ Final Check</td>\n        <td style=\"padding: 6px;\">Successfully retrieved N final summaries.</td>\n      </tr>\n    </tbody>\n  </table>\n\n  <hr>\n\n  <h3>üéØ Conclusion</h3>\n  <p>\n    This session service implementation satisfies <b>Sessions &amp; State Management</b> requirements, enabling:\n  </p>\n\n  <ul>\n    <li>Modular agent design</li>\n    <li>Seamless stage interoperability</li>\n    <li>Reproducible workflows</li>\n    <li>Efficient multi-agent pipelines powered by Gemma LLM</li>\n  </ul>\n\n</div>","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# IMPORTS\n# ============================================================\nfrom IPython.display import display, HTML\nimport asyncio\nimport aiohttp\nimport time\nimport json\n# Assuming keras_nlp is imported and the gemma_lm object exists\n# import keras_nlp\n\n# ============================================================\n# STYLED LOGGER\n# ============================================================\ndef log(text, level=\"info\"):\n    colors = {\n        \"info\": \"#e7f1ff\",\n        \"success\": \"#e7ffe7\",\n        \"warning\": \"#fff4e5\",\n        \"error\": \"#ffe5e5\",\n        \"header\": \"#dfe9ff\"\n    }\n    bg = colors.get(level, \"#f0f0f0\")\n    display(HTML(\n        f\"\"\"\n        <div style=\"\n            background:{bg};\n            padding:12px 18px;\n            border-radius:8px;\n            margin:6px 0;\n            font-family:'Segoe UI', sans-serif;\n            font-size:14px;\n        \">\n            {text}\n        </div>\n        \"\"\"\n    ))\n\n# ============================================================\n# SESSION SERVICE\n# ============================================================\nclass InMemorySessionService:\n    def __init__(self, session_id=\"news_agent_session\"):\n        self.session_id = session_id\n        self.memory = {}\n        log(f\"üß† Session started with ID: <b>{self.session_id}</b>\", \"header\")\n\n    def store_state(self, key, data):\n        self.memory[key] = data\n        log(f\"‚úî Stored state '<b>{key}</b>'\", \"success\")\n\n    def get_state(self, key):\n        if key in self.memory:\n            log(f\"üì• Retrieved state '<b>{key}</b>'\", \"info\")\n            return self.memory[key]\n        log(f\"‚ö† State key '<b>{key}</b>' not found\", \"warning\")\n        return None\n\n# ============================================================\n# STEP 1: ASYNC NEWS FETCHER WITH ERROR HANDLING\n# ============================================================\nURL = f\"https://newsapi.org/v2/everything?q=Artificial+Intelligence&language=en&sortBy=publishedAt&apiKey={NEWS_API_KEY}\"\n\nasync def fetch_news(session: InMemorySessionService):\n    log(\"<b>STEP 1:</b> Fetching AI news asynchronously‚Ä¶\", \"header\")\n    async with aiohttp.ClientSession() as client:\n        try:\n            async with client.get(URL) as resp:\n                data = await resp.json()\n\n                # Handle API errors\n                if data.get(\"status\") != \"ok\":\n                    log(f\"‚ùå NewsAPI error: {data.get('message', 'Unknown error')}\", \"error\")\n                    return\n\n                articles = data.get(\"articles\", [])\n                count = len(articles)\n\n                if count == 0:\n                    log(\"‚ö† NewsAPI returned 0 articles. Try a different query.\", \"warning\")\n                else:\n                    log(f\"‚è± {time.strftime('%Y-%m-%d %H:%M:%S')} ‚Äî fetched <b>{count}</b> articles\", \"info\")\n\n                session.store_state(\"raw_articles_list\", articles)\n                session.store_state(\"query_topic\", \"Artificial Intelligence\")\n                log(\"STEP 1 complete ‚Äî raw articles stored.\", \"success\")\n\n        except Exception as e:\n            log(f\"‚ùå Failed to fetch news: {e}\", \"error\")\n\n# ============================================================\n# STEP 2: ASYNC GEMMA LLM SUMMARIZER (OPTIMIZED & LIGHT)\n# ============================================================\nasync def summarize_article(gemma_lm, article, idx, semaphore):\n    \"\"\"\n    Summarizes a single article.\n    - Uses the pre-loaded gemma_lm model.\n    - Waits for the semaphore to get a \"slot\" to run.\n    - Truncates content to 2000 chars to save memory.\n    \"\"\"\n    # Wait for a \"slot\" to open up\n    async with semaphore:\n        log(f\"üü¢ Starting article {idx+1}...\", \"info\")\n        title = article.get(\"title\", \"Untitled Article\")\n        \n        # Truncate content to first 2000 chars to save memory\n        content = (article.get(\"content\") or article.get(\"description\") or \"No content available.\")[:2000]\n\n        prompt = f\"\"\"\nSummarize this AI news article in 2‚Äì3 sentences focusing on key AI insights:\n\nTitle: {title}\n\nContent:\n{content}\n\nReturn only the summary.\n        \"\"\"\n        max_len = 256\n        try:\n            # Use the pre-loaded gemma_lm object. No reloading!\n            output = gemma_lm.generate(prompt, max_length=max_len)\n\n            if isinstance(output, str):\n                summary = output\n            elif hasattr(output, \"generated_text\"):\n                summary = output.generated_text\n            elif hasattr(output, \"text\"):\n                summary = output.text\n            else:\n                summary = str(output)\n\n            log(f\"ü§ñ Gemma summarized article {idx+1}: <i>{title}</i>\", \"success\")\n            return summary.strip()\n\n        except Exception as e:\n            log(f\"‚ùå Gemma failed on article {idx+1}: {e}\", \"error\")\n            return None\n\nasync def run_llm_analysis(session: InMemorySessionService, gemma_lm):\n    log(\"<b>STEP 2:</b> Running Gemma LLM analysis (limited parallelism)...\", \"header\")\n    articles = session.get_state(\"raw_articles_list\")\n    if not articles or len(articles) == 0:\n        log(\"No articles found in session. Analysis skipped.\", \"warning\")\n        return\n\n    # --- LIGHTWEIGHT IMPROVEMENT ---\n    # Set a limit on concurrent tasks to avoid OOM errors\n    CONCURRENT_TASKS = 3 \n    semaphore = asyncio.Semaphore(CONCURRENT_TASKS)\n    log(f\"üö¶ Set concurrency limit to <b>{CONCURRENT_TASKS}</b> tasks.\", \"info\")\n    # -------------------------------\n\n    # Pass the semaphore to each task\n    tasks = [summarize_article(gemma_lm, article, idx, semaphore) for idx, article in enumerate(articles)]\n    summaries = await asyncio.gather(*tasks)\n    summaries = [s for s in summaries if s]  # remove None results\n\n    session.store_state(\"final_summaries\", summaries)\n    log(f\"STEP 2 complete ‚Äî <b>{len(summaries)}</b> summaries stored.\", \"success\")\n\n# ============================================================\n# MAIN PIPELINE (JUPYTER-FRIENDLY)\n# ============================================================\nasync def main_pipeline(gemma_lm):\n    agent_session = InMemorySessionService(\"newsbot_async_run\")\n    await fetch_news(agent_session)\n    await run_llm_analysis(agent_session, gemma_lm)\n\n    log(\"<b>Verification:</b> retrieving final summaries‚Ä¶\", \"header\")\n    final_summaries = agent_session.get_state(\"final_summaries\")\n    if final_summaries:\n        log(f\"üéâ Successfully processed <b>{len(final_summaries)}</b> articles.\", \"success\")\n    else:\n        log(\"‚ö† No summaries generated.\", \"warning\")\n\n# ============================================================\n# RUN PIPELINE IN JUPYTER\n# ============================================================\n# 1. Make sure you have loaded gemma_lm in a *previous* cell\n#    e.g.:\nimport keras_nlp\ngemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\n    \"gemma_instruct_2b_en\",\n    jit_compile=False,\n    dtype=\"bfloat16\" # Use bfloat16 to save memory!\n)\n\n# 2. Then, run this line:\nawait main_pipeline(gemma_lm)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T06:59:45.528557Z","iopub.execute_input":"2025-11-16T06:59:45.529435Z","iopub.status.idle":"2025-11-16T06:59:45.879588Z","shell.execute_reply.started":"2025-11-16T06:59:45.529399Z","shell.execute_reply":"2025-11-16T06:59:45.878579Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üéØ Conclusion: üì∞ NewsBot üóûÔ∏è A Fully Modular Analytics Engine ü§ñ\n\n<div style=\"\n  background: linear-gradient(135deg, #e0f7fa, #80deea, #e3f2fd);\n  border-radius: 16px;\n  padding: 25px;\n  box-shadow: 0 6px 18px rgba(0,0,0,0.15);\n  font-family: 'Segoe UI', sans-serif;\n  color: #0d47a1;\n  line-height: 1.6;\n\">\n\n<h2 style=\"text-align:center; color:#00695c; margin-top:0; letter-spacing:1px;\">\n‚ú® Conclusion: üì∞ NewsBot üóûÔ∏è A Fully Modular Analytics Engine ü§ñ\n</h2>\n\n<p>\nThe <strong>üì∞ NewsBot üóûÔ∏è</strong> project showcases the strength of <b>specialized, modular LLM pipelines</b> \nto automate the overwhelming task of news analysis. By combining the <b>Gemma LLM</b> with \ntools like News API, FAISS, and VADER, we built a system of independent modules that can:\n</p>\n\n<ul style=\"list-style:none; padding-left:0;\">\n  <li style=\"margin:12px 0; background:rgba(255,255,255,0.6); padding:12px; border-radius:8px;\">\n    üöÄ <b>Automate Acquisition & Digest Generation:</b> Fetches the latest news, analyzes with KerasNLP, and compiles polished <b>HTML digests</b>.\n  </li>\n  <li style=\"margin:12px 0; background:rgba(255,255,255,0.6); padding:12px; border-radius:8px;\">\n    üìä <b>Structure Unstructured Data:</b> Uses Gemma for <b>classification & structuring</b>, turning raw text into clean JSON powering the <b>Ethical News Dashboard</b>.\n  </li>\n  <li style=\"margin:12px 0; background:rgba(255,255,255,0.6); padding:12px; border-radius:8px;\">\n    üîç <b>Foundational RAG System:</b> Implements FAISS-based <b>retrieval-augmented generation</b> for context-aware Q&A over the article corpus.\n  </li>\n  <li style=\"margin:12px 0; background:rgba(255,255,255,0.6); padding:12px; border-radius:8px;\">\n    ü§ù <b>Validate Advanced Workflows:</b> Demonstrates <b>stateful communication</b> between modules via <code>InMemorySessionService</code>.\n  </li>\n</ul>\n\n<p>\nIn short, üì∞ NewsBot üóûÔ∏è goes beyond summarization to deliver a framework for <b>AI-powered intelligence gathering</b>. \nWhile optimization and embedding integration remain future goals, the notebook already stands as a \nproof-of-concept for building sophisticated, end-to-end analytical solutions. \nThe next phase will focus on scaling this backend into a professional application.\n</p>\n\n</div>","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}